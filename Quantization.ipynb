{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omar80-coder/Document-validation-Stamp-and-Signature-detection-using-Deep-Learning-/blob/main/Quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOnQfxuTr1rt",
        "outputId": "f1127576-15fe-45e7-84b1-5dc47e2aabe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_r6H5dSU1-lz",
        "outputId": "b5dd7b13-419c-473b-8151-2e60688b268f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.3.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.6-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (71.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.6-py3-none-any.whl (26 kB)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.6 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 torchmetrics-1.4.0.post0\n"
          ]
        }
      ],
      "source": [
        "pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7RU5M8Gt7at"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules\n",
        "import torch\n",
        "import torch.quantization\n",
        "import torchvision # Import torchvision\n",
        "from torchvision.models.detection import retinanet_resnet50_fpn_v2\n",
        "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
        "from torchvision.models.detection import RetinaNet_ResNet50_FPN_V2_Weights\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "#load the pre-trained model\n",
        "def create_model(num_classes=91):\n",
        "    model = torchvision.models.detection.retinanet_resnet50_fpn_v2( # Now torchvision is defined\n",
        "        weights=RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1\n",
        "    )\n",
        "    num_anchors = model.head.classification_head.num_anchors\n",
        "\n",
        "    model.head.classification_head = RetinaNetClassificationHead(\n",
        "        in_channels=256,\n",
        "        num_anchors=num_anchors,\n",
        "        num_classes=num_classes,\n",
        "        norm_layer=partial(torch.nn.GroupNorm, 32)\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Rfgx3NeV2_SY",
        "outputId": "a0b61376-b468-4855-b78d-37b8ec48240c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.3.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1->torchvision) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.1->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.1->torchvision) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufdaUIYsRUtd"
      },
      "outputs": [],
      "source": [
        "#function to load thr models\n",
        "def load_models(model_path, num_classes=3):\n",
        "    model=create_model(num_classes=3)\n",
        "    checkpoint=torch.load(model_path,map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1D8FtjvB0kUR",
        "outputId": "750abdd9-8f00-41ef-b534-b8fafbb2854f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/retinanet_resnet50_fpn_v2_coco-5905b1c5.pth\" to /root/.cache/torch/hub/checkpoints/retinanet_resnet50_fpn_v2_coco-5905b1c5.pth\n",
            "100%|██████████| 146M/146M [00:01<00:00, 152MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RetinaNet(\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0-2): 3 x Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (extra_blocks): LastLevelP6P7(\n",
              "        (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (anchor_generator): AnchorGenerator()\n",
              "  (head): RetinaNetHead(\n",
              "    (classification_head): RetinaNetClassificationHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (regression_head): RetinaNetRegressionHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (bbox_reg): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Load the model\n",
        "model_path='/content/drive/MyDrive/PFA INSAT - Invoice Validation/PFA Work - ML Invoice Validation/Advancements/Wissal/best_model.pth'\n",
        "model=create_model(num_classes=3)\n",
        "\n",
        "load_models(model_path,num_classes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "moqAFYlI3Vte",
        "outputId": "06e14f4b-5511-410c-95ec-8d108dd995e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RetinaNet(\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0-2): 3 x Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (extra_blocks): LastLevelP6P7(\n",
              "        (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (anchor_generator): AnchorGenerator()\n",
              "  (head): RetinaNetHead(\n",
              "    (classification_head): RetinaNetClassificationHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (regression_head): RetinaNetRegressionHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (bbox_reg): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_naWdBwe5jG-",
        "outputId": "007737e1-b9d2-461a-a1aa-0a3c1b9bd2b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RetinaNet(\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): QuantizedConv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.539108157157898, zero_point=127, padding=(3, 3), bias=False)\n",
              "      (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): QuantizedConv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), scale=5.531567096710205, zero_point=127, bias=False)\n",
              "          (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.9729180932044983, zero_point=142, padding=(1, 1), bias=False)\n",
              "          (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.6342097520828247, zero_point=154, bias=False)\n",
              "          (bn3): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=4.194687843322754, zero_point=168, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): QuantizedConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.99140465259552, zero_point=154, bias=False)\n",
              "          (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.1853617429733276, zero_point=117, padding=(1, 1), bias=False)\n",
              "          (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.9539124965667725, zero_point=138, bias=False)\n",
              "          (bn3): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): QuantizedConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), scale=1.1855723857879639, zero_point=130, bias=False)\n",
              "          (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=3.1814329624176025, zero_point=43, padding=(1, 1), bias=False)\n",
              "          (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=4.3815131187438965, zero_point=119, bias=False)\n",
              "          (bn3): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): QuantizedConv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), scale=5.632768154144287, zero_point=153, bias=False)\n",
              "          (bn1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), scale=12.729362487792969, zero_point=131, padding=(1, 1), bias=False)\n",
              "          (bn2): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=5.569505214691162, zero_point=128, bias=False)\n",
              "          (bn3): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=6.566168785095215, zero_point=125, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): QuantizedConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=13.545058250427246, zero_point=143, bias=False)\n",
              "          (bn1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=18.154056549072266, zero_point=136, padding=(1, 1), bias=False)\n",
              "          (bn2): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=9.236037254333496, zero_point=166, bias=False)\n",
              "          (bn3): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): QuantizedConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=10.234044075012207, zero_point=104, bias=False)\n",
              "          (bn1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=23.702367782592773, zero_point=162, padding=(1, 1), bias=False)\n",
              "          (bn2): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=6.194513320922852, zero_point=138, bias=False)\n",
              "          (bn3): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): QuantizedConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), scale=6.886038780212402, zero_point=148, bias=False)\n",
              "          (bn1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=5.056753158569336, zero_point=99, padding=(1, 1), bias=False)\n",
              "          (bn2): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): QuantizedConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), scale=3.0707285404205322, zero_point=139, bias=False)\n",
              "          (bn3): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): QuantizedConv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), scale=2.877692461013794, zero_point=226, bias=False)\n",
              "          (bn1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), scale=1.1107497215270996, zero_point=140, padding=(1, 1), bias=False)\n",
              "          (bn2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.3248777687549591, zero_point=131, bias=False)\n",
              "          (bn3): QuantizedBatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): QuantizedConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), scale=3.601915121078491, zero_point=139, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): QuantizedConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.7195019721984863, zero_point=160, bias=False)\n",
              "          (bn1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.284253478050232, zero_point=131, padding=(1, 1), bias=False)\n",
              "          (bn2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.3404628038406372, zero_point=126, bias=False)\n",
              "          (bn3): QuantizedBatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): QuantizedConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.8532676696777344, zero_point=145, bias=False)\n",
              "          (bn1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.5146468877792358, zero_point=120, padding=(1, 1), bias=False)\n",
              "          (bn2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.13321387767791748, zero_point=154, bias=False)\n",
              "          (bn3): QuantizedBatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): QuantizedConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.8812764286994934, zero_point=142, bias=False)\n",
              "          (bn1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.4447227716445923, zero_point=102, padding=(1, 1), bias=False)\n",
              "          (bn2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.13025066256523132, zero_point=129, bias=False)\n",
              "          (bn3): QuantizedBatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): QuantizedConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.7237303256988525, zero_point=161, bias=False)\n",
              "          (bn1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.36166438460350037, zero_point=136, padding=(1, 1), bias=False)\n",
              "          (bn2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.1236654669046402, zero_point=145, bias=False)\n",
              "          (bn3): QuantizedBatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): QuantizedConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.8552353382110596, zero_point=139, bias=False)\n",
              "          (bn1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.32266199588775635, zero_point=126, padding=(1, 1), bias=False)\n",
              "          (bn2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): QuantizedConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), scale=0.16200634837150574, zero_point=169, bias=False)\n",
              "          (bn3): QuantizedBatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): QuantizedConv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.5941450595855713, zero_point=168, bias=False)\n",
              "          (bn1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.3592606782913208, zero_point=141, padding=(1, 1), bias=False)\n",
              "          (bn2): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=0.24126873910427094, zero_point=128, bias=False)\n",
              "          (bn3): QuantizedBatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): QuantizedConv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), scale=0.5255149006843567, zero_point=164, bias=False)\n",
              "            (1): QuantizedBatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): QuantizedConv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.6988399624824524, zero_point=134, bias=False)\n",
              "          (bn1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.2929348349571228, zero_point=133, padding=(1, 1), bias=False)\n",
              "          (bn2): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=0.11263426393270493, zero_point=120, bias=False)\n",
              "          (bn3): QuantizedBatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): QuantizedConv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), scale=0.6913089156150818, zero_point=104, bias=False)\n",
              "          (bn1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.22942396998405457, zero_point=182, padding=(1, 1), bias=False)\n",
              "          (bn2): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): QuantizedConv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), scale=0.09276541322469711, zero_point=212, bias=False)\n",
              "          (bn3): QuantizedBatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): QuantizedConv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.5214006900787354, zero_point=130)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): QuantizedConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.4545159339904785, zero_point=128)\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): QuantizedConv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.20209261775016785, zero_point=128)\n",
              "        )\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=4.379683017730713, zero_point=131, padding=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.4719876050949097, zero_point=128, padding=(1, 1))\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.3717302680015564, zero_point=105, padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (extra_blocks): LastLevelP6P7(\n",
              "        (p6): QuantizedConv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.6804535388946533, zero_point=157, padding=(1, 1))\n",
              "        (p7): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), scale=1.021237850189209, zero_point=120, padding=(1, 1))\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (anchor_generator): AnchorGenerator()\n",
              "  (head): RetinaNetHead(\n",
              "    (classification_head): RetinaNetClassificationHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.8097870349884033, zero_point=149, padding=(1, 1), bias=False)\n",
              "          (1): QuantizedGroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.009549672715365887, zero_point=139, padding=(1, 1), bias=False)\n",
              "          (1): QuantizedGroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.010144590400159359, zero_point=135, padding=(1, 1), bias=False)\n",
              "          (1): QuantizedGroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.009712842293083668, zero_point=135, padding=(1, 1), bias=False)\n",
              "          (1): QuantizedGroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): QuantizedConv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), scale=0.021894089877605438, zero_point=255, padding=(1, 1))\n",
              "    )\n",
              "    (regression_head): RetinaNetRegressionHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=10.577848434448242, zero_point=126, padding=(1, 1), bias=False)\n",
              "          (1): QuantizedGroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.049655355513095856, zero_point=164, padding=(1, 1), bias=False)\n",
              "          (1): QuantizedGroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.129836305975914, zero_point=141, padding=(1, 1), bias=False)\n",
              "          (1): QuantizedGroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.9037509560585022, zero_point=178, padding=(1, 1), bias=False)\n",
              "          (1): QuantizedGroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (bbox_reg): QuantizedConv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), scale=0.03615579754114151, zero_point=125, padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#quantization of the model\n",
        "import torch.quantization\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# Define the data transformation\n",
        "import torchvision.transforms as T\n",
        "data_transforms = T.Compose([\n",
        "    T.Resize((800, 800)),\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "# Define a dummy input for the quantization process\n",
        "dummy_input = torch.randn(1, 3, 800, 800)\n",
        "\n",
        "# Specify the quantization configuration\n",
        "# Use 'qnnpack' for x86 CPU or leave it to None for default\n",
        "model.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
        "\n",
        "# Perform the static quantization\n",
        "torch.quantization.prepare(model, inplace=True)\n",
        "# Move the dummy input and model to the CUDA device if available\n",
        "if torch.cuda.is_available():\n",
        "    dummy_input = dummy_input.cuda()\n",
        "    model = model.cuda()\n",
        "model(dummy_input) # Calibration step\n",
        "torch.quantization.convert(model, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdf8D3Yc8CDZ",
        "outputId": "3fedda24-954f-436c-a0a9-0880a6863268"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_has_warned', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'anchor_generator', 'apply', 'backbone', 'bfloat16', 'box_coder', 'buffers', 'call_super_init', 'children', 'compile', 'compute_loss', 'cpu', 'cuda', 'detections_per_img', 'double', 'dump_patches', 'eager_outputs', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'head', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'nms_thresh', 'parameters', 'postprocess_detections', 'proposal_matcher', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'score_thresh', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'topk_candidates', 'train', 'training', 'transform', 'type', 'xpu', 'zero_grad']\n"
          ]
        }
      ],
      "source": [
        "# This will print a list of all methods and attributes of your model object\n",
        "print(dir(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTILDITOFihM",
        "outputId": "88c1891d-674c-4885-9c5e-bcff716e9ed6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Save to a temporary file first\n",
        "temp_path = 'temp_quantized_model.pth'\n",
        "torch.save(model.state_dict(), temp_path)\n",
        "\n",
        "# Mount Google Drive with write permissions (if not already mounted)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Copy to your desired location in Drive\n",
        "quantized_model_path = '/content/drive/MyDrive'\n",
        "!cp {temp_path} {quantized_model_path}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6d3WEncSRbz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "# Define the measure_inference_time function\n",
        "def measure_inference_time(model, input_data, num_runs=100):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Warm-up\n",
        "        for _ in range(10):  # Reduced warm-up runs for faster execution\n",
        "            model(input_data)\n",
        "\n",
        "        # Timing\n",
        "        start_time = time.time()\n",
        "        for _ in range(num_runs):\n",
        "            model(input_data)\n",
        "        end_time = time.time()\n",
        "\n",
        "    avg_time = (end_time - start_time) / num_runs\n",
        "    return avg_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "collapsed": true,
        "id": "YorvWPL2u077",
        "outputId": "6d28f9e8-6014-4fee-85ee-ae780af3c0d7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torchvision.models.detection.retinanet.RetinaNet</b><br/>def _wrapped_call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/retinanet.py</a>Implements RetinaNet.\n",
              "\n",
              "The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each\n",
              "image, and should be in 0-1 range. Different images can have different sizes.\n",
              "\n",
              "The behavior of the model changes depending on if it is in training or evaluation mode.\n",
              "\n",
              "During training, the model expects both the input tensors and targets (list of dictionary),\n",
              "containing:\n",
              "    - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with\n",
              "      ``0 &lt;= x1 &lt; x2 &lt;= W`` and ``0 &lt;= y1 &lt; y2 &lt;= H``.\n",
              "    - labels (Int64Tensor[N]): the class label for each ground-truth box\n",
              "\n",
              "The model returns a Dict[Tensor] during training, containing the classification and regression\n",
              "losses.\n",
              "\n",
              "During inference, the model requires only the input tensors, and returns the post-processed\n",
              "predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as\n",
              "follows:\n",
              "    - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with\n",
              "      ``0 &lt;= x1 &lt; x2 &lt;= W`` and ``0 &lt;= y1 &lt; y2 &lt;= H``.\n",
              "    - labels (Int64Tensor[N]): the predicted labels for each image\n",
              "    - scores (Tensor[N]): the scores for each prediction\n",
              "\n",
              "Args:\n",
              "    backbone (nn.Module): the network used to compute the features for the model.\n",
              "        It should contain an out_channels attribute, which indicates the number of output\n",
              "        channels that each feature map has (and it should be the same for all feature maps).\n",
              "        The backbone should return a single Tensor or an OrderedDict[Tensor].\n",
              "    num_classes (int): number of output classes of the model (including the background).\n",
              "    min_size (int): minimum size of the image to be rescaled before feeding it to the backbone\n",
              "    max_size (int): maximum size of the image to be rescaled before feeding it to the backbone\n",
              "    image_mean (Tuple[float, float, float]): mean values used for input normalization.\n",
              "        They are generally the mean values of the dataset on which the backbone has been trained\n",
              "        on\n",
              "    image_std (Tuple[float, float, float]): std values used for input normalization.\n",
              "        They are generally the std values of the dataset on which the backbone has been trained on\n",
              "    anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature\n",
              "        maps.\n",
              "    head (nn.Module): Module run on top of the feature pyramid.\n",
              "        Defaults to a module containing a classification and regression module.\n",
              "    score_thresh (float): Score threshold used for postprocessing the detections.\n",
              "    nms_thresh (float): NMS threshold used for postprocessing the detections.\n",
              "    detections_per_img (int): Number of best detections to keep after NMS.\n",
              "    fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be\n",
              "        considered as positive during training.\n",
              "    bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be\n",
              "        considered as negative during training.\n",
              "    topk_candidates (int): Number of best detections to keep before NMS.\n",
              "\n",
              "Example:\n",
              "\n",
              "    &gt;&gt;&gt; import torch\n",
              "    &gt;&gt;&gt; import torchvision\n",
              "    &gt;&gt;&gt; from torchvision.models.detection import RetinaNet\n",
              "    &gt;&gt;&gt; from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
              "    &gt;&gt;&gt; # load a pre-trained model for classification and return\n",
              "    &gt;&gt;&gt; # only the features\n",
              "    &gt;&gt;&gt; backbone = torchvision.models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).features\n",
              "    &gt;&gt;&gt; # RetinaNet needs to know the number of\n",
              "    &gt;&gt;&gt; # output channels in a backbone. For mobilenet_v2, it&#x27;s 1280,\n",
              "    &gt;&gt;&gt; # so we need to add it here\n",
              "    &gt;&gt;&gt; backbone.out_channels = 1280\n",
              "    &gt;&gt;&gt;\n",
              "    &gt;&gt;&gt; # let&#x27;s make the network generate 5 x 3 anchors per spatial\n",
              "    &gt;&gt;&gt; # location, with 5 different sizes and 3 different aspect\n",
              "    &gt;&gt;&gt; # ratios. We have a Tuple[Tuple[int]] because each feature\n",
              "    &gt;&gt;&gt; # map could potentially have different sizes and\n",
              "    &gt;&gt;&gt; # aspect ratios\n",
              "    &gt;&gt;&gt; anchor_generator = AnchorGenerator(\n",
              "    &gt;&gt;&gt;     sizes=((32, 64, 128, 256, 512),),\n",
              "    &gt;&gt;&gt;     aspect_ratios=((0.5, 1.0, 2.0),)\n",
              "    &gt;&gt;&gt; )\n",
              "    &gt;&gt;&gt;\n",
              "    &gt;&gt;&gt; # put the pieces together inside a RetinaNet model\n",
              "    &gt;&gt;&gt; model = RetinaNet(backbone,\n",
              "    &gt;&gt;&gt;                   num_classes=2,\n",
              "    &gt;&gt;&gt;                   anchor_generator=anchor_generator)\n",
              "    &gt;&gt;&gt; model.eval()\n",
              "    &gt;&gt;&gt; x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
              "    &gt;&gt;&gt; predictions = model(x)</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 323);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "torchvision.models.detection.retinanet.RetinaNet"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHzNdkgS0l7N"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "04K6mOXhF34X",
        "outputId": "3af2a69c-33be-4dc3-ddad-417feed6e1ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 133MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Model Architecture:\n",
            "RetinaNet(\n",
            "  (backbone): BackboneWithFPN(\n",
            "    (body): IntermediateLayerGetter(\n",
            "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (layer1): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer2): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer3): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (4): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (5): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer4): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (fpn): FeaturePyramidNetwork(\n",
            "      (inner_blocks): ModuleList(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (2): Conv2dNormActivation(\n",
            "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (layer_blocks): ModuleList(\n",
            "        (0-2): 3 x Conv2dNormActivation(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (extra_blocks): LastLevelP6P7(\n",
            "        (p6): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (anchor_generator): AnchorGenerator()\n",
            "  (head): RetinaNetHead(\n",
            "    (classification_head): RetinaNetClassificationHead(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Conv2dNormActivation(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (cls_logits): Conv2d(256, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "    (regression_head): RetinaNetRegressionHead(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Conv2dNormActivation(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          (1): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (bbox_reg): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (transform): GeneralizedRCNNTransform(\n",
            "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Paths to the models\n",
        "original_model_path = '/content/drive/MyDrive/PFA INSAT - Invoice Validation/PFA Work - ML Invoice Validation/Advancements/Wissal/best_model.pth'\n",
        "quantized_model_path = '/content/drive/MyDrive/temp_quantized_model.pth'\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Load the original model\n",
        "original_model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=False, num_classes=3)\n",
        "\n",
        "# Print the model architecture to check for discrepancies\n",
        "print(\"Original Model Architecture:\")\n",
        "print(original_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wael-FkGOz3"
      },
      "outputs": [],
      "source": [
        "# Load the state_dict from the checkpoint and map to CPU\n",
        "checkpoint = torch.load(original_model_path, map_location=torch.device('cpu'))\n",
        "\n",
        "# Extract the model's state_dict\n",
        "model_state_dict = checkpoint['model_state_dict']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Wkibmn2rGrE5",
        "outputId": "e381e8a8-46fe-4993-c5e7-d79fcacbfa07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys in the loaded state_dict:\n",
            "backbone.body.conv1.weight\n",
            "backbone.body.bn1.weight\n",
            "backbone.body.bn1.bias\n",
            "backbone.body.bn1.running_mean\n",
            "backbone.body.bn1.running_var\n",
            "backbone.body.bn1.num_batches_tracked\n",
            "backbone.body.layer1.0.conv1.weight\n",
            "backbone.body.layer1.0.bn1.weight\n",
            "backbone.body.layer1.0.bn1.bias\n",
            "backbone.body.layer1.0.bn1.running_mean\n",
            "backbone.body.layer1.0.bn1.running_var\n",
            "backbone.body.layer1.0.bn1.num_batches_tracked\n",
            "backbone.body.layer1.0.conv2.weight\n",
            "backbone.body.layer1.0.bn2.weight\n",
            "backbone.body.layer1.0.bn2.bias\n",
            "backbone.body.layer1.0.bn2.running_mean\n",
            "backbone.body.layer1.0.bn2.running_var\n",
            "backbone.body.layer1.0.bn2.num_batches_tracked\n",
            "backbone.body.layer1.0.conv3.weight\n",
            "backbone.body.layer1.0.bn3.weight\n",
            "backbone.body.layer1.0.bn3.bias\n",
            "backbone.body.layer1.0.bn3.running_mean\n",
            "backbone.body.layer1.0.bn3.running_var\n",
            "backbone.body.layer1.0.bn3.num_batches_tracked\n",
            "backbone.body.layer1.0.downsample.0.weight\n",
            "backbone.body.layer1.0.downsample.1.weight\n",
            "backbone.body.layer1.0.downsample.1.bias\n",
            "backbone.body.layer1.0.downsample.1.running_mean\n",
            "backbone.body.layer1.0.downsample.1.running_var\n",
            "backbone.body.layer1.0.downsample.1.num_batches_tracked\n",
            "backbone.body.layer1.1.conv1.weight\n",
            "backbone.body.layer1.1.bn1.weight\n",
            "backbone.body.layer1.1.bn1.bias\n",
            "backbone.body.layer1.1.bn1.running_mean\n",
            "backbone.body.layer1.1.bn1.running_var\n",
            "backbone.body.layer1.1.bn1.num_batches_tracked\n",
            "backbone.body.layer1.1.conv2.weight\n",
            "backbone.body.layer1.1.bn2.weight\n",
            "backbone.body.layer1.1.bn2.bias\n",
            "backbone.body.layer1.1.bn2.running_mean\n",
            "backbone.body.layer1.1.bn2.running_var\n",
            "backbone.body.layer1.1.bn2.num_batches_tracked\n",
            "backbone.body.layer1.1.conv3.weight\n",
            "backbone.body.layer1.1.bn3.weight\n",
            "backbone.body.layer1.1.bn3.bias\n",
            "backbone.body.layer1.1.bn3.running_mean\n",
            "backbone.body.layer1.1.bn3.running_var\n",
            "backbone.body.layer1.1.bn3.num_batches_tracked\n",
            "backbone.body.layer1.2.conv1.weight\n",
            "backbone.body.layer1.2.bn1.weight\n",
            "backbone.body.layer1.2.bn1.bias\n",
            "backbone.body.layer1.2.bn1.running_mean\n",
            "backbone.body.layer1.2.bn1.running_var\n",
            "backbone.body.layer1.2.bn1.num_batches_tracked\n",
            "backbone.body.layer1.2.conv2.weight\n",
            "backbone.body.layer1.2.bn2.weight\n",
            "backbone.body.layer1.2.bn2.bias\n",
            "backbone.body.layer1.2.bn2.running_mean\n",
            "backbone.body.layer1.2.bn2.running_var\n",
            "backbone.body.layer1.2.bn2.num_batches_tracked\n",
            "backbone.body.layer1.2.conv3.weight\n",
            "backbone.body.layer1.2.bn3.weight\n",
            "backbone.body.layer1.2.bn3.bias\n",
            "backbone.body.layer1.2.bn3.running_mean\n",
            "backbone.body.layer1.2.bn3.running_var\n",
            "backbone.body.layer1.2.bn3.num_batches_tracked\n",
            "backbone.body.layer2.0.conv1.weight\n",
            "backbone.body.layer2.0.bn1.weight\n",
            "backbone.body.layer2.0.bn1.bias\n",
            "backbone.body.layer2.0.bn1.running_mean\n",
            "backbone.body.layer2.0.bn1.running_var\n",
            "backbone.body.layer2.0.bn1.num_batches_tracked\n",
            "backbone.body.layer2.0.conv2.weight\n",
            "backbone.body.layer2.0.bn2.weight\n",
            "backbone.body.layer2.0.bn2.bias\n",
            "backbone.body.layer2.0.bn2.running_mean\n",
            "backbone.body.layer2.0.bn2.running_var\n",
            "backbone.body.layer2.0.bn2.num_batches_tracked\n",
            "backbone.body.layer2.0.conv3.weight\n",
            "backbone.body.layer2.0.bn3.weight\n",
            "backbone.body.layer2.0.bn3.bias\n",
            "backbone.body.layer2.0.bn3.running_mean\n",
            "backbone.body.layer2.0.bn3.running_var\n",
            "backbone.body.layer2.0.bn3.num_batches_tracked\n",
            "backbone.body.layer2.0.downsample.0.weight\n",
            "backbone.body.layer2.0.downsample.1.weight\n",
            "backbone.body.layer2.0.downsample.1.bias\n",
            "backbone.body.layer2.0.downsample.1.running_mean\n",
            "backbone.body.layer2.0.downsample.1.running_var\n",
            "backbone.body.layer2.0.downsample.1.num_batches_tracked\n",
            "backbone.body.layer2.1.conv1.weight\n",
            "backbone.body.layer2.1.bn1.weight\n",
            "backbone.body.layer2.1.bn1.bias\n",
            "backbone.body.layer2.1.bn1.running_mean\n",
            "backbone.body.layer2.1.bn1.running_var\n",
            "backbone.body.layer2.1.bn1.num_batches_tracked\n",
            "backbone.body.layer2.1.conv2.weight\n",
            "backbone.body.layer2.1.bn2.weight\n",
            "backbone.body.layer2.1.bn2.bias\n",
            "backbone.body.layer2.1.bn2.running_mean\n",
            "backbone.body.layer2.1.bn2.running_var\n",
            "backbone.body.layer2.1.bn2.num_batches_tracked\n",
            "backbone.body.layer2.1.conv3.weight\n",
            "backbone.body.layer2.1.bn3.weight\n",
            "backbone.body.layer2.1.bn3.bias\n",
            "backbone.body.layer2.1.bn3.running_mean\n",
            "backbone.body.layer2.1.bn3.running_var\n",
            "backbone.body.layer2.1.bn3.num_batches_tracked\n",
            "backbone.body.layer2.2.conv1.weight\n",
            "backbone.body.layer2.2.bn1.weight\n",
            "backbone.body.layer2.2.bn1.bias\n",
            "backbone.body.layer2.2.bn1.running_mean\n",
            "backbone.body.layer2.2.bn1.running_var\n",
            "backbone.body.layer2.2.bn1.num_batches_tracked\n",
            "backbone.body.layer2.2.conv2.weight\n",
            "backbone.body.layer2.2.bn2.weight\n",
            "backbone.body.layer2.2.bn2.bias\n",
            "backbone.body.layer2.2.bn2.running_mean\n",
            "backbone.body.layer2.2.bn2.running_var\n",
            "backbone.body.layer2.2.bn2.num_batches_tracked\n",
            "backbone.body.layer2.2.conv3.weight\n",
            "backbone.body.layer2.2.bn3.weight\n",
            "backbone.body.layer2.2.bn3.bias\n",
            "backbone.body.layer2.2.bn3.running_mean\n",
            "backbone.body.layer2.2.bn3.running_var\n",
            "backbone.body.layer2.2.bn3.num_batches_tracked\n",
            "backbone.body.layer2.3.conv1.weight\n",
            "backbone.body.layer2.3.bn1.weight\n",
            "backbone.body.layer2.3.bn1.bias\n",
            "backbone.body.layer2.3.bn1.running_mean\n",
            "backbone.body.layer2.3.bn1.running_var\n",
            "backbone.body.layer2.3.bn1.num_batches_tracked\n",
            "backbone.body.layer2.3.conv2.weight\n",
            "backbone.body.layer2.3.bn2.weight\n",
            "backbone.body.layer2.3.bn2.bias\n",
            "backbone.body.layer2.3.bn2.running_mean\n",
            "backbone.body.layer2.3.bn2.running_var\n",
            "backbone.body.layer2.3.bn2.num_batches_tracked\n",
            "backbone.body.layer2.3.conv3.weight\n",
            "backbone.body.layer2.3.bn3.weight\n",
            "backbone.body.layer2.3.bn3.bias\n",
            "backbone.body.layer2.3.bn3.running_mean\n",
            "backbone.body.layer2.3.bn3.running_var\n",
            "backbone.body.layer2.3.bn3.num_batches_tracked\n",
            "backbone.body.layer3.0.conv1.weight\n",
            "backbone.body.layer3.0.bn1.weight\n",
            "backbone.body.layer3.0.bn1.bias\n",
            "backbone.body.layer3.0.bn1.running_mean\n",
            "backbone.body.layer3.0.bn1.running_var\n",
            "backbone.body.layer3.0.bn1.num_batches_tracked\n",
            "backbone.body.layer3.0.conv2.weight\n",
            "backbone.body.layer3.0.bn2.weight\n",
            "backbone.body.layer3.0.bn2.bias\n",
            "backbone.body.layer3.0.bn2.running_mean\n",
            "backbone.body.layer3.0.bn2.running_var\n",
            "backbone.body.layer3.0.bn2.num_batches_tracked\n",
            "backbone.body.layer3.0.conv3.weight\n",
            "backbone.body.layer3.0.bn3.weight\n",
            "backbone.body.layer3.0.bn3.bias\n",
            "backbone.body.layer3.0.bn3.running_mean\n",
            "backbone.body.layer3.0.bn3.running_var\n",
            "backbone.body.layer3.0.bn3.num_batches_tracked\n",
            "backbone.body.layer3.0.downsample.0.weight\n",
            "backbone.body.layer3.0.downsample.1.weight\n",
            "backbone.body.layer3.0.downsample.1.bias\n",
            "backbone.body.layer3.0.downsample.1.running_mean\n",
            "backbone.body.layer3.0.downsample.1.running_var\n",
            "backbone.body.layer3.0.downsample.1.num_batches_tracked\n",
            "backbone.body.layer3.1.conv1.weight\n",
            "backbone.body.layer3.1.bn1.weight\n",
            "backbone.body.layer3.1.bn1.bias\n",
            "backbone.body.layer3.1.bn1.running_mean\n",
            "backbone.body.layer3.1.bn1.running_var\n",
            "backbone.body.layer3.1.bn1.num_batches_tracked\n",
            "backbone.body.layer3.1.conv2.weight\n",
            "backbone.body.layer3.1.bn2.weight\n",
            "backbone.body.layer3.1.bn2.bias\n",
            "backbone.body.layer3.1.bn2.running_mean\n",
            "backbone.body.layer3.1.bn2.running_var\n",
            "backbone.body.layer3.1.bn2.num_batches_tracked\n",
            "backbone.body.layer3.1.conv3.weight\n",
            "backbone.body.layer3.1.bn3.weight\n",
            "backbone.body.layer3.1.bn3.bias\n",
            "backbone.body.layer3.1.bn3.running_mean\n",
            "backbone.body.layer3.1.bn3.running_var\n",
            "backbone.body.layer3.1.bn3.num_batches_tracked\n",
            "backbone.body.layer3.2.conv1.weight\n",
            "backbone.body.layer3.2.bn1.weight\n",
            "backbone.body.layer3.2.bn1.bias\n",
            "backbone.body.layer3.2.bn1.running_mean\n",
            "backbone.body.layer3.2.bn1.running_var\n",
            "backbone.body.layer3.2.bn1.num_batches_tracked\n",
            "backbone.body.layer3.2.conv2.weight\n",
            "backbone.body.layer3.2.bn2.weight\n",
            "backbone.body.layer3.2.bn2.bias\n",
            "backbone.body.layer3.2.bn2.running_mean\n",
            "backbone.body.layer3.2.bn2.running_var\n",
            "backbone.body.layer3.2.bn2.num_batches_tracked\n",
            "backbone.body.layer3.2.conv3.weight\n",
            "backbone.body.layer3.2.bn3.weight\n",
            "backbone.body.layer3.2.bn3.bias\n",
            "backbone.body.layer3.2.bn3.running_mean\n",
            "backbone.body.layer3.2.bn3.running_var\n",
            "backbone.body.layer3.2.bn3.num_batches_tracked\n",
            "backbone.body.layer3.3.conv1.weight\n",
            "backbone.body.layer3.3.bn1.weight\n",
            "backbone.body.layer3.3.bn1.bias\n",
            "backbone.body.layer3.3.bn1.running_mean\n",
            "backbone.body.layer3.3.bn1.running_var\n",
            "backbone.body.layer3.3.bn1.num_batches_tracked\n",
            "backbone.body.layer3.3.conv2.weight\n",
            "backbone.body.layer3.3.bn2.weight\n",
            "backbone.body.layer3.3.bn2.bias\n",
            "backbone.body.layer3.3.bn2.running_mean\n",
            "backbone.body.layer3.3.bn2.running_var\n",
            "backbone.body.layer3.3.bn2.num_batches_tracked\n",
            "backbone.body.layer3.3.conv3.weight\n",
            "backbone.body.layer3.3.bn3.weight\n",
            "backbone.body.layer3.3.bn3.bias\n",
            "backbone.body.layer3.3.bn3.running_mean\n",
            "backbone.body.layer3.3.bn3.running_var\n",
            "backbone.body.layer3.3.bn3.num_batches_tracked\n",
            "backbone.body.layer3.4.conv1.weight\n",
            "backbone.body.layer3.4.bn1.weight\n",
            "backbone.body.layer3.4.bn1.bias\n",
            "backbone.body.layer3.4.bn1.running_mean\n",
            "backbone.body.layer3.4.bn1.running_var\n",
            "backbone.body.layer3.4.bn1.num_batches_tracked\n",
            "backbone.body.layer3.4.conv2.weight\n",
            "backbone.body.layer3.4.bn2.weight\n",
            "backbone.body.layer3.4.bn2.bias\n",
            "backbone.body.layer3.4.bn2.running_mean\n",
            "backbone.body.layer3.4.bn2.running_var\n",
            "backbone.body.layer3.4.bn2.num_batches_tracked\n",
            "backbone.body.layer3.4.conv3.weight\n",
            "backbone.body.layer3.4.bn3.weight\n",
            "backbone.body.layer3.4.bn3.bias\n",
            "backbone.body.layer3.4.bn3.running_mean\n",
            "backbone.body.layer3.4.bn3.running_var\n",
            "backbone.body.layer3.4.bn3.num_batches_tracked\n",
            "backbone.body.layer3.5.conv1.weight\n",
            "backbone.body.layer3.5.bn1.weight\n",
            "backbone.body.layer3.5.bn1.bias\n",
            "backbone.body.layer3.5.bn1.running_mean\n",
            "backbone.body.layer3.5.bn1.running_var\n",
            "backbone.body.layer3.5.bn1.num_batches_tracked\n",
            "backbone.body.layer3.5.conv2.weight\n",
            "backbone.body.layer3.5.bn2.weight\n",
            "backbone.body.layer3.5.bn2.bias\n",
            "backbone.body.layer3.5.bn2.running_mean\n",
            "backbone.body.layer3.5.bn2.running_var\n",
            "backbone.body.layer3.5.bn2.num_batches_tracked\n",
            "backbone.body.layer3.5.conv3.weight\n",
            "backbone.body.layer3.5.bn3.weight\n",
            "backbone.body.layer3.5.bn3.bias\n",
            "backbone.body.layer3.5.bn3.running_mean\n",
            "backbone.body.layer3.5.bn3.running_var\n",
            "backbone.body.layer3.5.bn3.num_batches_tracked\n",
            "backbone.body.layer4.0.conv1.weight\n",
            "backbone.body.layer4.0.bn1.weight\n",
            "backbone.body.layer4.0.bn1.bias\n",
            "backbone.body.layer4.0.bn1.running_mean\n",
            "backbone.body.layer4.0.bn1.running_var\n",
            "backbone.body.layer4.0.bn1.num_batches_tracked\n",
            "backbone.body.layer4.0.conv2.weight\n",
            "backbone.body.layer4.0.bn2.weight\n",
            "backbone.body.layer4.0.bn2.bias\n",
            "backbone.body.layer4.0.bn2.running_mean\n",
            "backbone.body.layer4.0.bn2.running_var\n",
            "backbone.body.layer4.0.bn2.num_batches_tracked\n",
            "backbone.body.layer4.0.conv3.weight\n",
            "backbone.body.layer4.0.bn3.weight\n",
            "backbone.body.layer4.0.bn3.bias\n",
            "backbone.body.layer4.0.bn3.running_mean\n",
            "backbone.body.layer4.0.bn3.running_var\n",
            "backbone.body.layer4.0.bn3.num_batches_tracked\n",
            "backbone.body.layer4.0.downsample.0.weight\n",
            "backbone.body.layer4.0.downsample.1.weight\n",
            "backbone.body.layer4.0.downsample.1.bias\n",
            "backbone.body.layer4.0.downsample.1.running_mean\n",
            "backbone.body.layer4.0.downsample.1.running_var\n",
            "backbone.body.layer4.0.downsample.1.num_batches_tracked\n",
            "backbone.body.layer4.1.conv1.weight\n",
            "backbone.body.layer4.1.bn1.weight\n",
            "backbone.body.layer4.1.bn1.bias\n",
            "backbone.body.layer4.1.bn1.running_mean\n",
            "backbone.body.layer4.1.bn1.running_var\n",
            "backbone.body.layer4.1.bn1.num_batches_tracked\n",
            "backbone.body.layer4.1.conv2.weight\n",
            "backbone.body.layer4.1.bn2.weight\n",
            "backbone.body.layer4.1.bn2.bias\n",
            "backbone.body.layer4.1.bn2.running_mean\n",
            "backbone.body.layer4.1.bn2.running_var\n",
            "backbone.body.layer4.1.bn2.num_batches_tracked\n",
            "backbone.body.layer4.1.conv3.weight\n",
            "backbone.body.layer4.1.bn3.weight\n",
            "backbone.body.layer4.1.bn3.bias\n",
            "backbone.body.layer4.1.bn3.running_mean\n",
            "backbone.body.layer4.1.bn3.running_var\n",
            "backbone.body.layer4.1.bn3.num_batches_tracked\n",
            "backbone.body.layer4.2.conv1.weight\n",
            "backbone.body.layer4.2.bn1.weight\n",
            "backbone.body.layer4.2.bn1.bias\n",
            "backbone.body.layer4.2.bn1.running_mean\n",
            "backbone.body.layer4.2.bn1.running_var\n",
            "backbone.body.layer4.2.bn1.num_batches_tracked\n",
            "backbone.body.layer4.2.conv2.weight\n",
            "backbone.body.layer4.2.bn2.weight\n",
            "backbone.body.layer4.2.bn2.bias\n",
            "backbone.body.layer4.2.bn2.running_mean\n",
            "backbone.body.layer4.2.bn2.running_var\n",
            "backbone.body.layer4.2.bn2.num_batches_tracked\n",
            "backbone.body.layer4.2.conv3.weight\n",
            "backbone.body.layer4.2.bn3.weight\n",
            "backbone.body.layer4.2.bn3.bias\n",
            "backbone.body.layer4.2.bn3.running_mean\n",
            "backbone.body.layer4.2.bn3.running_var\n",
            "backbone.body.layer4.2.bn3.num_batches_tracked\n",
            "backbone.fpn.inner_blocks.0.0.weight\n",
            "backbone.fpn.inner_blocks.0.0.bias\n",
            "backbone.fpn.inner_blocks.1.0.weight\n",
            "backbone.fpn.inner_blocks.1.0.bias\n",
            "backbone.fpn.inner_blocks.2.0.weight\n",
            "backbone.fpn.inner_blocks.2.0.bias\n",
            "backbone.fpn.layer_blocks.0.0.weight\n",
            "backbone.fpn.layer_blocks.0.0.bias\n",
            "backbone.fpn.layer_blocks.1.0.weight\n",
            "backbone.fpn.layer_blocks.1.0.bias\n",
            "backbone.fpn.layer_blocks.2.0.weight\n",
            "backbone.fpn.layer_blocks.2.0.bias\n",
            "backbone.fpn.extra_blocks.p6.weight\n",
            "backbone.fpn.extra_blocks.p6.bias\n",
            "backbone.fpn.extra_blocks.p7.weight\n",
            "backbone.fpn.extra_blocks.p7.bias\n",
            "head.classification_head.conv.0.0.weight\n",
            "head.classification_head.conv.0.1.weight\n",
            "head.classification_head.conv.0.1.bias\n",
            "head.classification_head.conv.1.0.weight\n",
            "head.classification_head.conv.1.1.weight\n",
            "head.classification_head.conv.1.1.bias\n",
            "head.classification_head.conv.2.0.weight\n",
            "head.classification_head.conv.2.1.weight\n",
            "head.classification_head.conv.2.1.bias\n",
            "head.classification_head.conv.3.0.weight\n",
            "head.classification_head.conv.3.1.weight\n",
            "head.classification_head.conv.3.1.bias\n",
            "head.classification_head.cls_logits.weight\n",
            "head.classification_head.cls_logits.bias\n",
            "head.regression_head.conv.0.0.weight\n",
            "head.regression_head.conv.0.1.weight\n",
            "head.regression_head.conv.0.1.bias\n",
            "head.regression_head.conv.1.0.weight\n",
            "head.regression_head.conv.1.1.weight\n",
            "head.regression_head.conv.1.1.bias\n",
            "head.regression_head.conv.2.0.weight\n",
            "head.regression_head.conv.2.1.weight\n",
            "head.regression_head.conv.2.1.bias\n",
            "head.regression_head.conv.3.0.weight\n",
            "head.regression_head.conv.3.1.weight\n",
            "head.regression_head.conv.3.1.bias\n",
            "head.regression_head.bbox_reg.weight\n",
            "head.regression_head.bbox_reg.bias\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Print the keys of the state_dict\n",
        "print(\"Keys in the loaded state_dict:\")\n",
        "for key in model_state_dict.keys():\n",
        "    print(key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxWNm4ojJScX",
        "outputId": "9ea96b1a-f168-40e5-c77c-15e020181283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing keys: []\n",
            "Unexpected keys: []\n"
          ]
        }
      ],
      "source": [
        "# Load the state_dict but exclude the layers with mismatched sizes\n",
        "pretrained_dict = model_state_dict\n",
        "model_dict = original_model.state_dict()\n",
        "\n",
        "# Filter out mismatched layers\n",
        "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
        "\n",
        "# Overwrite entries in the existing state_dict\n",
        "model_dict.update(pretrained_dict)\n",
        "\n",
        "# Load the new state_dict\n",
        "original_model.load_state_dict(model_dict)\n",
        "\n",
        "# Print the missing and unexpected keys after loading the filtered state_dict\n",
        "missing_keys, unexpected_keys = original_model.load_state_dict(model_dict, strict=False)\n",
        "print(f\"Missing keys: {missing_keys}\")\n",
        "print(f\"Unexpected keys: {unexpected_keys}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XjlavOEcZhR"
      },
      "source": [
        "**create_model & Load models & measure inference time function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7JedjSplJswN"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules\n",
        "import torch\n",
        "import torch.quantization\n",
        "import torchvision\n",
        "from torchvision.models.detection import retinanet_resnet50_fpn_v2\n",
        "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
        "from torchvision.models.detection import RetinaNet_ResNet50_FPN_V2_Weights\n",
        "from functools import partial\n",
        "\n",
        "# Load the pre-trained model\n",
        "def create_model(num_classes=91):\n",
        "    model = torchvision.models.detection.retinanet_resnet50_fpn_v2(\n",
        "        weights=RetinaNet_ResNet50_FPN_V2_Weights.COCO_V1\n",
        "    )\n",
        "    num_anchors = model.head.classification_head.num_anchors\n",
        "\n",
        "    model.head.classification_head = RetinaNetClassificationHead(\n",
        "        in_channels=256,\n",
        "        num_anchors=num_anchors,\n",
        "        num_classes=num_classes,\n",
        "        norm_layer=partial(torch.nn.GroupNorm, 32)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Function to load the model\n",
        "def load_models(model_path, num_classes=3):\n",
        "    model = create_model(num_classes=num_classes)\n",
        "    checkpoint = torch.load(model_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Load the model\n",
        "model_path = '/content/drive/MyDrive/PFA INSAT - Invoice Validation/PFA Work - ML Invoice Validation/Advancements/Wissal/best_model.pth'\n",
        "model = load_models(model_path, num_classes=3)\n",
        "\n",
        "\n",
        "\n",
        "# Define the measure_inference_time function\n",
        "import time\n",
        "def measure_inference_time(model, input_data, num_runs=100):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Warm-up\n",
        "        for _ in range(10):  # Reduced warm-up runs for faster execution\n",
        "            model(input_data)\n",
        "\n",
        "        # Timing\n",
        "        start_time = time.time()\n",
        "        for _ in range(num_runs):\n",
        "            model(input_data)\n",
        "        end_time = time.time()\n",
        "\n",
        "    avg_time = (end_time - start_time) / num_runs\n",
        "    return avg_time\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snJCc8IycCqK"
      },
      "source": [
        "**Quantization&Saving to the quantized model to a specific path **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3M7J0gzqb-1P"
      },
      "outputs": [],
      "source": [
        "# Quantization of the model\n",
        "import torch.quantization\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# Define the data transformation\n",
        "data_transforms = T.Compose([\n",
        "    T.Resize((800, 800)),\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "# Define a dummy input for the quantization process\n",
        "dummy_input = torch.randn(1, 3, 800, 800)\n",
        "\n",
        "# Specify the quantization configuration\n",
        "model.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
        "\n",
        "# Perform the static quantization\n",
        "torch.quantization.prepare(model, inplace=True)\n",
        "\n",
        "# Move the dummy input and model to the CUDA device if available\n",
        "if torch.cuda.is_available():\n",
        "    dummy_input = dummy_input.cuda()\n",
        "    model = model.cuda()\n",
        "model(dummy_input)  # Calibration step\n",
        "torch.quantization.convert(model, inplace=True)\n",
        "\n",
        "# Save to a temporary file first\n",
        "temp_path = 'temp_quantized_model.pth'\n",
        "torch.save(model.state_dict(), temp_path)\n",
        "\n",
        "# Mount Google Drive with write permissions (if not already mounted)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Copy to your desired location in Drive\n",
        "quantized_model_path = '/content/drive/MyDrive/temp_quantized_model.pth'\n",
        "!cp {temp_path} {quantized_model_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogpIQMKcbUYv"
      },
      "source": [
        "**Loading original model /model_state_dict**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VL5Syzb0bSdx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Paths to the models\n",
        "original_model_path = '/content/drive/MyDrive/PFA INSAT - Invoice Validation/PFA Work - ML Invoice Validation/Advancements/Wissal/best_model.pth'\n",
        "quantized_model_path = '/content/drive/MyDrive/temp_quantized_model.pth'\n",
        "\n",
        "# Load the original model\n",
        "original_model = create_model(num_classes=3)\n",
        "\n",
        "# Load the state_dict from the checkpoint and map to CPU\n",
        "checkpoint = torch.load(original_model_path, map_location=torch.device('cpu'))\n",
        "\n",
        "# Extract the model's state_dict\n",
        "model_state_dict = checkpoint['model_state_dict']\n",
        "\n",
        "# Load the state_dict into the original model\n",
        "original_model.load_state_dict(model_state_dict)\n",
        "\n",
        "# Print the model architecture to check for discrepancies\n",
        "print(\"Original Model Architecture:\")\n",
        "print(original_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "EEhY94mMQShL",
        "outputId": "2c4f78c1-f141-422f-c93b-d3c04c2e8bbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Model Inference Time: 0.069022 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:382: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  device=storage.device,\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for RetinaNet:\n\tUnexpected key(s) in state_dict: \"backbone.body.conv1.bias\", \"backbone.body.conv1.scale\", \"backbone.body.conv1.zero_point\", \"backbone.body.bn1.scale\", \"backbone.body.bn1.zero_point\", \"backbone.body.layer1.0.conv1.bias\", \"backbone.body.layer1.0.conv1.scale\", \"backbone.body.layer1.0.conv1.zero_point\", \"backbone.body.layer1.0.bn1.scale\", \"backbone.body.layer1.0.bn1.zero_point\", \"backbone.body.layer1.0.conv2.bias\", \"backbone.body.layer1.0.conv2.scale\", \"backbone.body.layer1.0.conv2.zero_point\", \"backbone.body.layer1.0.bn2.scale\", \"backbone.body.layer1.0.bn2.zero_point\", \"backbone.body.layer1.0.conv3.bias\", \"backbone.body.layer1.0.conv3.scale\", \"backbone.body.layer1.0.conv3.zero_point\", \"backbone.body.layer1.0.bn3.scale\", \"backbone.body.layer1.0.bn3.zero_point\", \"backbone.body.layer1.0.downsample.0.bias\", \"backbone.body.layer1.0.downsample.0.scale\", \"backbone.body.layer1.0.downsample.0.zero_point\", \"backbone.body.layer1.0.downsample.1.scale\", \"backbone.body.layer1.0.downsample.1.zero_point\", \"backbone.body.layer1.1.conv1.bias\", \"backbone.body.layer1.1.conv1.scale\", \"backbone.body.layer1.1.conv1.zero_point\", \"backbone.body.layer1.1.bn1.scale\", \"backbone.body.layer1.1.bn1.zero_point\", \"backbone.body.layer1.1.conv2.bias\", \"backbone.body.layer1.1.conv2.scale\", \"backbone.body.layer1.1.conv2.zero_point\", \"backbone.body.layer1.1.bn2.scale\", \"backbone.body.layer1.1.bn2.zero_point\", \"backbone.body.layer1.1.conv3.bias\", \"backbone.body.layer1.1.conv3.scale\", \"backbone.body.layer1.1.conv3.zero_point\", \"backbone.body.layer1.1.bn3.scale\", \"backbone.body.layer1.1.bn3.zero_point\", \"backbone.body.layer1.2.conv1.bias\", \"backbone.body.layer1.2.conv1.scale\", \"backbone.body.layer1.2.conv1.zero_point\", \"backbone.body.layer1.2.bn1.scale\", \"backbone.body.layer1.2.bn1.zero_point\", \"backbone.body.layer1.2.conv2.bias\", \"backbone.body.layer1.2.conv2.scale\", \"backbone.body.layer1.2.conv2.zero_point\", \"backbone.body.layer1.2.bn2.scale\", \"backbone.body.layer1.2.bn2.zero_point\", \"backbone.body.layer1.2.conv3.bias\", \"backbone.body.layer1.2.conv3.scale\", \"backbone.body.layer1.2.conv3.zero_point\", \"backbone.body.layer1.2.bn3.scale\", \"backbone.body.layer1.2.bn3.zero_point\", \"backbone.body.layer2.0.conv1.bias\", \"backbone.body.layer2.0.conv1.scale\", \"backbone.body.layer2.0.conv1.zero_point\", \"backbone.body.layer2.0.bn1.scale\", \"backbone.body.layer2.0.bn1.zero_point\", \"backbone.body.layer2.0.conv2.bias\", \"backbone.body.layer2.0.conv2.scale\", \"backbone.body.layer2.0.conv2.zero_point\", \"backbone.body.layer2.0.bn2.scale\", \"backbone.body.layer2.0.bn2.zero_point\", \"backbone.body.layer2.0.conv3.bias\", \"backbone.body.layer2.0.conv3.scale\", \"backbone.body.layer2.0.conv3.zero_point\", \"backbone.body.layer2.0.bn3.scale\", \"backbone.body.layer2.0.bn3.zero_point\", \"backbone.body.layer2.0.downsample.0.bias\", \"backbone.body.layer2.0.downsample.0.scale\", \"backbone.body.layer2.0.downsample.0.zero_point\", \"backbone.body.layer2.0.downsample.1.scale\", \"backbone.body.layer2.0.downsample.1.zero_point\", \"backbone.body.layer2.1.conv1.bias\", \"backbone.body.layer2.1.conv1.scale\", \"backbone.body.layer2.1.conv1.zero_point\", \"backbone.body.layer2.1.bn1.scale\", \"backbone.body.layer2.1.bn1.zero_point\", \"backbone.body.layer2.1.conv2.bias\", \"backbone.body.layer2.1.conv2.scale\", \"backbone.body.layer2.1.conv2.zero_point\", \"backbone.body.layer2.1.bn2.scale\", \"backbone.body.layer2.1.bn2.zero_point\", \"backbone.body.layer2.1.conv3.bias\", \"backbone.body.layer2.1.conv3.scale\", \"backbone.body.layer2.1.conv3.zero_point\", \"backbone.body.layer2.1.bn3.scale\", \"backbone.body.layer2.1.bn3.zero_point\", \"backbone.body.layer2.2.conv1.bias\", \"backbone.body.layer2.2.conv1.scale\", \"backbone.body.layer2.2.conv1.zero_point\", \"backbone.body.layer2.2.bn1.scale\", \"backbone.body.layer2.2.bn1.zero_point\", \"backbone.body.layer2.2.conv2.bias\", \"backbone.body.layer2.2.conv2.scale\", \"backbone.body.layer2.2.conv2.zero_point\", \"backbone.body.layer2.2.bn2.scale\", \"backbone.body.layer2.2.bn2.zero_point\", \"backbone.body.layer2.2.conv3.bias\", \"backbone.body.layer2.2.conv3.scale\", \"backbone.body.layer2.2.conv3.zero_point\", \"backbone.body.layer2.2.bn3.scale\", \"backbone.body.layer2.2.bn3.zero_point\", \"backbone.body.layer2.3.conv1.bias\", \"backbone.body.layer2.3.conv1.scale\", \"backbone.body.layer2.3.conv1.zero_point\", \"backbone.body.layer2.3.bn1.scale\", \"backbone.body.layer2.3.bn1.zero_point\", \"backbone.body.layer2.3.conv2.bias\", \"backbone.body.layer2.3.conv2.scale\", \"backbone.body.layer2.3.conv2.zero_point\", \"backbone.body.layer2.3.bn2.scale\", \"backbone.body.layer2.3.bn2.zero_point\", \"backbone.body.layer2.3.conv3.bias\", \"backbone.body.layer2.3.conv3.scale\", \"backbone.body.layer2.3.conv3.zero_point\", \"backbone.body.layer2.3.bn3.scale\", \"backbone.body.layer2.3.bn3.zero_point\", \"backbone.body.layer3.0.conv1.bias\", \"backbone.body.layer3.0.conv1.scale\", \"backbone.body.layer3.0.conv1.zero_point\", \"backbone.body.layer3.0.bn1.scale\", \"backbone.body.layer3.0.bn1.zero_point\", \"backbone.body.layer3.0.conv2.bias\", \"backbone.body.layer3.0.conv2.scale\", \"backbone.body.layer3.0.conv2.zero_point\", \"backbone.body.layer3.0.bn2.scale\", \"backbone.body.layer3.0.bn2.zero_point\", \"backbone.body.layer3.0.conv3.bias\", \"backbone.body.layer3.0.conv3.scale\", \"backbone.body.layer3.0.conv3.zero_point\", \"backbone.body.layer3.0.bn3.scale\", \"backbone.body.layer3.0.bn3.zero_point\", \"backbone.body.layer3.0.downsample.0.bias\", \"backbone.body.layer3.0.downsample.0.scale\", \"backbone.body.layer3.0.downsample.0.zero_point\", \"backbone.body.layer3.0.downsample.1.scale\", \"backbone.body.layer3.0.downsample.1.zero_point\", \"backbone.body.layer3.1.conv1.bias\", \"backbone.body.layer3.1.conv1.scale\", \"backbone.body.layer3.1.conv1.zero_point\", \"backbone.body.layer3.1.bn1.scale\", \"backbone.body.layer3.1.bn1.zero_point\", \"backbone.body.layer3.1.conv2.bias\", \"backbone.body.layer3.1.conv2.scale\", \"backbone.body.layer3.1.conv2.zero_point\", \"backbone.body.layer3.1.bn2.scale\", \"backbone.body.layer3.1.bn2.zero_point\", \"backbone.body.layer3.1.conv3.bias\", \"backbone.body.layer3.1.conv3.scale\", \"backbone.body.layer3.1.conv3.zero_point\", \"backbone.body.layer3.1.bn3.scale\", \"backbone.body.layer3.1.bn3.zero_point\", \"backbone.body.layer3.2.conv1.bias\", \"backbone.body.layer3.2.conv1.scale\", \"backbone.body.layer3.2.conv1.zero_point\", \"backbone.body.layer3.2.bn1.scale\", \"backbone.body.layer3.2.bn1.zero_point\", \"backbone.body.layer3.2.conv2.bias\", \"backbone.body.layer3.2.conv2.scale\", \"backbone.body.layer3.2.conv2.zero_point\", \"backbone.body.layer3.2.bn2.scale\", \"backbone.body.layer3.2.bn2.zero_point\", \"backbone.body.layer3.2.conv3.bias\", \"backbone.body.layer3.2.conv3.scale\", \"backbone.body.layer3.2.conv3.zero_point\", \"backbone.body.layer3.2.bn3.scale\", \"backbone.body.layer3.2.bn3.zero_point\", \"backbone.body.layer3.3.conv1.bias\", \"backbone.body.layer3.3.conv1.scale\", \"backbone.body.layer3.3.conv1.zero_point\", \"backbone.body.layer3.3.bn1.scale\", \"backbone.body.layer3.3.bn1.zero_point\", \"backbone.body.layer3.3.conv2.bias\", \"backbone.body.layer3.3.conv2.scale\", \"backbone.body.layer3.3.conv2.zero_point\", \"backbone.body.layer3.3.bn2.scale\", \"backbone.body.layer3.3.bn2.zero_point\", \"backbone.body.layer3.3.conv3.bias\", \"backbone.body.layer3.3.conv3.scale\", \"backbone.body.layer3.3.conv3.zero_point\", \"backbone.body.layer3.3.bn3.scale\", \"backbone.body.layer3.3.bn3.zero_point\", \"backbone.body.layer3.4.conv1.bias\", \"backbone.body.layer3.4.conv1.scale\", \"backbone.body.layer3.4.conv1.zero_point\", \"backbone.body.layer3.4.bn1.scale\", \"backbone.body.layer3.4.bn1.zero_point\", \"backbone.body.layer3.4.conv2.bias\", \"backbone.body.layer3.4.conv2.scale\", \"backbone.body.layer3.4.conv2.zero_point\", \"backbone.body.layer3.4.bn2.scale\", \"backbone.body.layer3.4.bn2.zero_point\", \"backbone.body.layer3.4.conv3.bias\", \"backbone.body.layer3.4.conv3.scale\", \"backbone.body.layer3.4.conv3.zero_point\", \"backbone.body.layer3.4.bn3.scale\", \"backbone.body.layer3.4.bn3.zero_point\", \"backbone.body.layer3.5.conv1.bias\", \"backbone.body.layer3.5.conv1.scale\", \"backbone.body.layer3.5.conv1.zero_point\", \"backbone.body.layer3.5.bn1.scale\", \"backbone.body.layer3.5.bn1.zero_point\", \"backbone.body.layer3.5.conv2.bias\", \"backbone.body.layer3.5.conv2.scale\", \"backbone.body.layer3.5.conv2.zero_point\", \"backbone.body.layer3.5.bn2.scale\", \"backbone.body.layer3.5.bn2.zero_point\", \"backbone.body.layer3.5.conv3.bias\", \"backbone.body.layer3.5.conv3.scale\", \"backbone.body.layer3.5.conv3.zero_point\", \"backbone.body.layer3.5.bn3.scale\", \"backbone.body.layer3.5.bn3.zero_point\", \"backbone.body.layer4.0.conv1.bias\", \"backbone.body.layer4.0.conv1.scale\", \"backbone.body.layer4.0.conv1.zero_point\", \"backbone.body.layer4.0.bn1.scale\", \"backbone.body.layer4.0.bn1.zero_point\", \"backbone.body.layer4.0.conv2.bias\", \"backbone.body.layer4.0.conv2.scale\", \"backbone.body.layer4.0.conv2.zero_point\", \"backbone.body.layer4.0.bn2.scale\", \"backbone.body.layer4.0.bn2.zero_point\", \"backbone.body.layer4.0.conv3.bias\", \"backbone.body.layer4.0.conv3.scale\", \"backbone.body.layer4.0.conv3.zero_point\", \"backbone.body.layer4.0.bn3.scale\", \"backbone.body.layer4.0.bn3.zero_point\", \"backbone.body.layer4.0.downsample.0.bias\", \"backbone.body.layer4.0.downsample.0.scale\", \"backbone.body.layer4.0.downsample.0.zero_point\", \"backbone.body.layer4.0.downsample.1.scale\", \"backbone.body.layer4.0.downsample.1.zero_point\", \"backbone.body.layer4.1.conv1.bias\", \"backbone.body.layer4.1.conv1.scale\", \"backbone.body.layer4.1.conv1.zero_point\", \"backbone.body.layer4.1.bn1.scale\", \"backbone.body.layer4.1.bn1.zero_point\", \"backbone.body.layer4.1.conv2.bias\", \"backbone.body.layer4.1.conv2.scale\", \"backbone.body.layer4.1.conv2.zero_point\", \"backbone.body.layer4.1.bn2.scale\", \"backbone.body.layer4.1.bn2.zero_point\", \"backbone.body.layer4.1.conv3.bias\", \"backbone.body.layer4.1.conv3.scale\", \"backbone.body.layer4.1.conv3.zero_point\", \"backbone.body.layer4.1.bn3.scale\", \"backbone.body.layer4.1.bn3.zero_point\", \"backbone.body.layer4.2.conv1.bias\", \"backbone.body.layer4.2.conv1.scale\", \"backbone.body.layer4.2.conv1.zero_point\", \"backbone.body.layer4.2.bn1.scale\", \"backbone.body.layer4.2.bn1.zero_point\", \"backbone.body.layer4.2.conv2.bias\", \"backbone.body.layer4.2.conv2.scale\", \"backbone.body.layer4.2.conv2.zero_point\", \"backbone.body.layer4.2.bn2.scale\", \"backbone.body.layer4.2.bn2.zero_point\", \"backbone.body.layer4.2.conv3.bias\", \"backbone.body.layer4.2.conv3.scale\", \"backbone.body.layer4.2.conv3.zero_point\", \"backbone.body.layer4.2.bn3.scale\", \"backbone.body.layer4.2.bn3.zero_point\", \"backbone.fpn.inner_blocks.0.0.scale\", \"backbone.fpn.inner_blocks.0.0.zero_point\", \"backbone.fpn.inner_blocks.1.0.scale\", \"backbone.fpn.inner_blocks.1.0.zero_point\", \"backbone.fpn.inner_blocks.2.0.scale\", \"backbone.fpn.inner_blocks.2.0.zero_point\", \"backbone.fpn.layer_blocks.0.0.scale\", \"backbone.fpn.layer_blocks.0.0.zero_point\", \"backbone.fpn.layer_blocks.1.0.scale\", \"backbone.fpn.layer_blocks.1.0.zero_point\", \"backbone.fpn.layer_blocks.2.0.scale\", \"backbone.fpn.layer_blocks.2.0.zero_point\", \"backbone.fpn.extra_blocks.p6.scale\", \"backbone.fpn.extra_blocks.p6.zero_point\", \"backbone.fpn.extra_blocks.p7.scale\", \"backbone.fpn.extra_blocks.p7.zero_point\", \"head.classification_head.conv.0.0.bias\", \"head.classification_head.conv.0.0.scale\", \"head.classification_head.conv.0.0.zero_point\", \"head.classification_head.conv.0.1.scale\", \"head.classification_head.conv.0.1.zero_point\", \"head.classification_head.conv.1.0.bias\", \"head.classification_head.conv.1.0.scale\", \"head.classification_head.conv.1.0.zero_point\", \"head.classification_head.conv.1.1.scale\", \"head.classification_head.conv.1.1.zero_point\", \"head.classification_head.conv.2.0.bias\", \"head.classification_head.conv.2.0.scale\", \"head.classification_head.conv.2.0.zero_point\", \"head.classification_head.conv.2.1.scale\", \"head.classification_head.conv.2.1.zero_point\", \"head.classification_head.conv.3.0.bias\", \"head.classification_head.conv.3.0.scale\", \"head.classification_head.conv.3.0.zero_point\", \"head.classification_head.conv.3.1.scale\", \"head.classification_head.conv.3.1.zero_point\", \"head.classification_head.cls_logits.scale\", \"head.classification_head.cls_logits.zero_point\", \"head.regression_head.conv.0.0.bias\", \"head.regression_head.conv.0.0.scale\", \"head.regression_head.conv.0.0.zero_point\", \"head.regression_head.conv.0.1.scale\", \"head.regression_head.conv.0.1.zero_point\", \"head.regression_head.conv.1.0.bias\", \"head.regression_head.conv.1.0.scale\", \"head.regression_head.conv.1.0.zero_point\", \"head.regression_head.conv.1.1.scale\", \"head.regression_head.conv.1.1.zero_point\", \"head.regression_head.conv.2.0.bias\", \"head.regression_head.conv.2.0.scale\", \"head.regression_head.conv.2.0.zero_point\", \"head.regression_head.conv.2.1.scale\", \"head.regression_head.conv.2.1.zero_point\", \"head.regression_head.conv.3.0.bias\", \"head.regression_head.conv.3.0.scale\", \"head.regression_head.conv.3.0.zero_point\", \"head.regression_head.conv.3.1.scale\", \"head.regression_head.conv.3.1.zero_point\", \"head.regression_head.bbox_reg.scale\", \"head.regression_head.bbox_reg.zero_point\". \n\tsize mismatch for backbone.body.conv1.weight: copying a param with shape torch.Size([64, 4, 7, 7]) from checkpoint, the shape in current model is torch.Size([64, 3, 7, 7]).\n\tWhile copying the parameter named \"backbone.body.layer1.0.conv1.weight\", whose dimensions in the model are torch.Size([64, 64, 1, 1]) and whose dimensions in the checkpoint are torch.Size([64, 64, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer1.0.conv2.weight\", whose dimensions in the model are torch.Size([64, 64, 3, 3]) and whose dimensions in the checkpoint are torch.Size([64, 64, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer1.0.conv3.weight\", whose dimensions in the model are torch.Size([256, 64, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 64, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer1.0.downsample.0.weight\", whose dimensions in the model are torch.Size([256, 64, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 64, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer1.1.conv1.weight\", whose dimensions in the model are torch.Size([64, 256, 1, 1]) and whose dimensions in the checkpoint are torch.Size([64, 256, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer1.1.conv2.weight\", whose dimensions in the model are torch.Size([64, 64, 3, 3]) and whose dimensions in the checkpoint are torch.Size([64, 64, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer1.1.conv3.weight\", whose dimensions in the model are torch.Size([256, 64, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 64, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer1.2.conv1.weight\", whose dimensions in the model are torch.Size([64, 256, 1, 1]) and whose dimensions in the checkpoint are torch.Size([64, 256, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer1.2.conv2.weight\", whose dimensions in the model are torch.Size([64, 64, 3, 3]) and whose dimensions in the checkpoint are torch.Size([64, 64, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer1.2.conv3.weight\", whose dimensions in the model are torch.Size([256, 64, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 64, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.0.conv1.weight\", whose dimensions in the model are torch.Size([128, 256, 1, 1]) and whose dimensions in the checkpoint are torch.Size([128, 256, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.0.conv2.weight\", whose dimensions in the model are torch.Size([128, 128, 3, 3]) and whose dimensions in the checkpoint are torch.Size([128, 128, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.0.conv3.weight\", whose dimensions in the model are torch.Size([512, 128, 1, 1]) and whose dimensions in the checkpoint are torch.Size([512, 128, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.0.downsample.0.weight\", whose dimensions in the model are torch.Size([512, 256, 1, 1]) and whose dimensions in the checkpoint are torch.Size([512, 256, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.1.conv1.weight\", whose dimensions in the model are torch.Size([128, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([128, 512, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.1.conv2.weight\", whose dimensions in the model are torch.Size([128, 128, 3, 3]) and whose dimensions in the checkpoint are torch.Size([128, 128, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.1.conv3.weight\", whose dimensions in the model are torch.Size([512, 128, 1, 1]) and whose dimensions in the checkpoint are torch.Size([512, 128, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.2.conv1.weight\", whose dimensions in the model are torch.Size([128, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([128, 512, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.2.conv2.weight\", whose dimensions in the model are torch.Size([128, 128, 3, 3]) and whose dimensions in the checkpoint are torch.Size([128, 128, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.2.conv3.weight\", whose dimensions in the model are torch.Size([512, 128, 1, 1]) and whose dimensions in the checkpoint are torch.Size([512, 128, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.3.conv1.weight\", whose dimensions in the model are torch.Size([128, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([128, 512, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.3.conv2.weight\", whose dimensions in the model are torch.Size([128, 128, 3, 3]) and whose dimensions in the checkpoint are torch.Size([128, 128, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.3.conv3.weight\", whose dimensions in the model are torch.Size([512, 128, 1, 1]) and whose dimensions in the checkpoint are torch.Size([512, 128, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.0.conv1.weight\", whose dimensions in the model are torch.Size([256, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 512, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.0.conv2.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.0.conv3.weight\", whose dimensions in the model are torch.Size([1024, 256, 1, 1]) and whose dimensions in the checkpoint are torch.Size([1024, 256, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.0.downsample.0.weight\", whose dimensions in the model are torch.Size([1024, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([1024, 512, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.1.conv1.weight\", whose dimensions in the model are torch.Size([256, 1024, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 1024, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.1.conv2.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.1.conv3.weight\", whose dimensions in the model are torch.Size([1024, 256, 1, 1]) and whose dimensions in the checkpoint are torch.Size([1024, 256, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.2.conv1.weight\", whose dimensions in the model are torch.Size([256, 1024, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 1024, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.2.conv2.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.2.conv3.weight\", whose dimensions in the model are torch.Size([1024, 256, 1, 1]) and whose dimensions in the checkpoint are torch.Size([1024, 256, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.3.conv1.weight\", whose dimensions in the model are torch.Size([256, 1024, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 1024, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.3.conv2.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.3.conv3.weight\", whose dimensions in the model are torch.Size([1024, 256, 1, 1]) and whose dimensions in the checkpoint are torch.Size([1024, 256, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.4.conv1.weight\", whose dimensions in the model are torch.Size([256, 1024, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 1024, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.4.conv2.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.4.conv3.weight\", whose dimensions in the model are torch.Size([1024, 256, 1, 1]) and whose dimensions in the checkpoint are torch.Size([1024, 256, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.5.conv1.weight\", whose dimensions in the model are torch.Size([256, 1024, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 1024, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.5.conv2.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.5.conv3.weight\", whose dimensions in the model are torch.Size([1024, 256, 1, 1]) and whose dimensions in the checkpoint are torch.Size([1024, 256, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer4.0.conv1.weight\", whose dimensions in the model are torch.Size([512, 1024, 1, 1]) and whose dimensions in the checkpoint are torch.Size([512, 1024, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer4.0.conv2.weight\", whose dimensions in the model are torch.Size([512, 512, 3, 3]) and whose dimensions in the checkpoint are torch.Size([512, 512, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer4.0.conv3.weight\", whose dimensions in the model are torch.Size([2048, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([2048, 512, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer4.0.downsample.0.weight\", whose dimensions in the model are torch.Size([2048, 1024, 1, 1]) and whose dimensions in the checkpoint are torch.Size([2048, 1024, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer4.1.conv1.weight\", whose dimensions in the model are torch.Size([512, 2048, 1, 1]) and whose dimensions in the checkpoint are torch.Size([512, 2048, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer4.1.conv2.weight\", whose dimensions in the model are torch.Size([512, 512, 3, 3]) and whose dimensions in the checkpoint are torch.Size([512, 512, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer4.1.conv3.weight\", whose dimensions in the model are torch.Size([2048, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([2048, 512, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer4.2.conv1.weight\", whose dimensions in the model are torch.Size([512, 2048, 1, 1]) and whose dimensions in the checkpoint are torch.Size([512, 2048, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer4.2.conv2.weight\", whose dimensions in the model are torch.Size([512, 512, 3, 3]) and whose dimensions in the checkpoint are torch.Size([512, 512, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer4.2.conv3.weight\", whose dimensions in the model are torch.Size([2048, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([2048, 512, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.fpn.inner_blocks.0.0.weight\", whose dimensions in the model are torch.Size([256, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 512, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.fpn.inner_blocks.1.0.weight\", whose dimensions in the model are torch.Size([256, 1024, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 1024, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.fpn.inner_blocks.2.0.weight\", whose dimensions in the model are torch.Size([256, 2048, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 2048, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.fpn.layer_blocks.0.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.fpn.layer_blocks.1.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.fpn.layer_blocks.2.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.fpn.extra_blocks.p6.weight\", whose dimensions in the model are torch.Size([256, 2048, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 2048, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.fpn.extra_blocks.p7.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"head.classification_head.conv.0.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"head.classification_head.conv.1.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"head.classification_head.conv.2.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"head.classification_head.conv.3.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tsize mismatch for head.classification_head.cls_logits.weight: copying a param with shape torch.Size([28, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([27, 256, 3, 3]).\n\tsize mismatch for head.classification_head.cls_logits.bias: copying a param with shape torch.Size([28]) from checkpoint, the shape in current model is torch.Size([27]).\n\tWhile copying the parameter named \"head.regression_head.conv.0.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"head.regression_head.conv.1.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"head.regression_head.conv.2.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"head.regression_head.conv.3.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"head.regression_head.bbox_reg.weight\", whose dimensions in the model are torch.Size([36, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([36, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-0d13b3e9f43f>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the quantized model from the saved state_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mquantized_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mquantized_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Measure inference time for the quantized model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2189\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2190\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2191\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for RetinaNet:\n\tUnexpected key(s) in state_dict: \"backbone.body.conv1.bias\", \"backbone.body.conv1.scale\", \"backbone.body.conv1.zero_point\", \"backbone.body.bn1.scale\", \"backbone.body.bn1.zero_point\", \"backbone.body.layer1.0.conv1.bias\", \"backbone.body.layer1.0.conv1.scale\", \"backbone.body.layer1.0.conv1.zero_point\", \"backbone.body.layer1.0.bn1.scale\", \"backbone.body.layer1.0.bn1.zero_point\", \"backbone.body.layer1.0.conv2.bias\", \"backbone.body.layer1.0.conv2.scale\", \"backbone.body.layer1.0.conv2.zero_point\", \"backbone.body.layer1.0.bn2.scale\", \"backbone.body.layer1.0.bn2.zero_point\", \"backbone.body.layer1.0.conv3.bias\", \"backbone.body.layer1.0.conv3.scale\", \"backbone.body.layer1.0.conv3.zero_point\", \"backbone.body.layer1.0.bn3.scale\", \"backbone.body.layer1.0.bn3.zero_point\", \"backbone.body.layer1.0.downsample.0.bias\", \"backbone.body.layer1.0.downsample.0.scale\", \"backbone.body.layer1.0.downsample.0.zero_point\", \"backbone.body.layer1.0.downsample.1.scale\", \"backbone.body.layer1.0.downsample.1.zero_point\", \"backbone.body.layer1.1.conv1.bias\", \"backbone.body.layer1.1.conv1.scale\", \"backbone.body.layer1.1.conv1.zero_point\", \"backbone.body.layer1.1.bn1.scale\", \"backbone.body.layer1.1.bn1.zero_point\", \"backbone.body.layer1.1.conv2.bias\", \"backbone.body.layer1.1.conv2.scale\", \"backbone.body.layer1.1.conv2.zero_point\", \"backbone.body.layer1.1.bn2.scale\", \"backbone.body.layer1.1.bn2.zero_point\", \"backbone.body.layer1.1.conv3.bias\", \"backbone.body.layer1.1.conv3.scale\", \"backbone.body.layer1.1.conv3.zero_point\", \"backbone.body.layer1.1.bn3.scale\", \"backbone.body.layer1.1.bn3.zero_point\", \"backbone.body.layer1.2.conv1.bias\", \"backbone.body.layer1.2.conv1.scale\", \"backbone.body.layer1.2.conv1.zero_point\", \"backbone.body.layer1.2.bn1.scale\", \"backbone.body.layer1.2.bn1.zero_point\", \"backbone.body.layer1.2.conv2.bias\", \"backbone.body.layer1.2.conv2.scale\", \"backbone.body.layer1.2.conv2.zero_point\", \"backbone.body.layer1.2.bn2.scale\", \"backbone.body.layer1.2.bn2.zero_point\", \"backbone.body.layer1.2.conv3.bias\", \"backbone.body.layer1.2.conv3.scale\", \"backbone.body.layer1.2.conv3.zero_point\", \"backbone.body.layer1.2.bn3.scale\", \"backbone.body.layer1.2.bn3.zero_point\", \"backbone.body.layer2.0.conv1.bias\", \"backbone.body.layer2.0.conv1.scale\", \"backbone.body.layer2.0.conv1.zero_point\", \"backbone.body.layer2.0.bn1.scale\", \"backbone.body.layer2.0.bn1.zero_point\", \"backbone.body.layer2.0.conv2.bias\", \"backbone.body.layer2.0.conv2.scale\", \"backbone.body.layer2.0.conv2.zero_point\", \"backbone.body.layer2.0.bn2.scale\", \"backbone.body.layer2.0.bn2.zero_point\", \"backbone.body.layer2.0.conv3.bias\", \"backbone.body.layer2.0.conv3.scale\", \"backbone.body.layer2.0.conv3.zero_point\", \"backbone.body.layer2.0.bn3.scale\", \"backbone.body.layer2.0.bn3.zero_point\", \"backbone.body.layer2.0.downsample.0.bias\", \"backbone.body.layer2.0.downsample.0.scale\", \"backbone.body.layer2.0.downsample.0.zero_point\", \"backbone.body.layer2.0.downsample.1.scale\", \"backbone.body.layer2.0.downsample.1.zero_point\", \"backbone.body.layer2.1.conv1.bias\", \"backbone.body.layer2.1.conv1.scale\", \"backbone.body.layer2.1.conv1.zero_point\", \"backbone.body.layer2.1.bn1.scale\", \"backbone.body.layer2.1.bn1.zero_point\", \"backbone.body.layer2.1.conv2.bias\", \"backbone.body.layer2.1.conv2.scale\", \"backbone.body.layer2.1.conv2.zero_point\", \"backbone.body.layer2.1.bn2.scale\", \"backbone.body.layer2.1.bn2.zero_point\", \"backbone.body.layer2.1.conv3.bias\", \"backbone.body.layer2.1.conv3.scale\", \"backbone.body.layer2.1.conv3.zero_point\", \"backbone.body.layer2.1.bn3.scale\", \"backbone.body.layer2.1.bn3.zero_point\", \"backbone.body.layer2.2.conv1.bias\", \"backbone.body.layer2.2.conv1.scale\", \"backbone.body.layer2.2.conv1.zero_point\", \"backbone.body.layer2.2.bn1.scale\", \"backbone.body.layer2.2.bn1.zero_point\", \"backbone.body.layer2.2.conv2.bias\", \"backbone.body.layer2.2.conv2.scale\", \"backbone.body.layer2.2.conv2.zero_point\", \"backbone.body.layer2.2.bn2.scale\", \"backbone.body.layer2.2.bn2.zero_point\", \"backbone.body.layer2.2.conv3.bias\", \"backbone.body.layer2.2.conv3.scale\", \"backbone.body.layer2.2.conv3.zero_point\", \"backbone.body.layer2.2.bn3.scale\", \"backbone.body.layer2.2.bn3.zero_point\", \"backbone.body.layer2.3.conv1.bias\", \"backbone.body.layer2.3.conv1.scale\", \"backbone.body.layer2.3.conv1.zero_point\", \"backbone.body.layer2.3.bn1.scale\", \"backbone.body.layer2.3.bn1.zero_point\", \"backbone.body.layer2.3.conv2.bias\", \"backbone.body.layer2.3.conv2.scale\", \"backbone.body.layer2.3.conv2.zero_point\", \"backbone.body.layer2.3.bn2.scale\", \"backbone.body.layer2.3.bn2.zero_point\", \"backbone.body.layer2.3.conv3.bias\", \"backbone.body.layer2.3.conv3.scale\", \"backbone.body.layer2.3.conv3.zero_point\", \"backbone.body.layer2.3.bn3.scale\", \"backbone.body.layer2.3.bn3.zero_point\", \"backbone.body.layer3.0.conv1.bias\", \"backbone.body.layer3.0.conv1.scale\", \"backbone.body.layer3.0.conv1.zero_point\", \"backbone.body.layer3.0.bn1.scale\", \"backbone.body.layer3.0.bn1.zero_point\", \"backbone.body.layer3.0.conv2.bias\", \"backbone.body.layer3.0.conv2.scale\", \"backbone.body.layer3.0.conv2.zero_point\", \"backbone.body.layer3.0.bn2.scale\", \"backbone.body.layer3.0.bn2.zero_point\", \"backbone.body.layer3.0.conv3.bias\", \"backbone.body.layer3.0.conv3.scale\", \"backbone.body.layer3.0.conv3.zero_point\", \"backbone.body.layer3.0.bn3.scale\", \"backbone.body.layer3.0.bn3.zero_point\", \"backbone.body.layer3.0.downsample.0.bias\", \"backbone.body.layer3.0.downsample.0.scale\", \"backbone.body.layer3.0.downsample.0.zero_point\", \"backbone.body.layer3.0.downsample.1.scale\", \"backbone.body.layer3.0.downsample.1.zero_point\", \"backbone.body.layer3.1.conv1.bias\", \"backbone.body.layer3.1.conv1.scale\", \"backbone.body.layer3.1.conv1.zero_point\", \"backbone.body.layer3.1.bn1.scale\", \"backbone.body.layer3.1.bn1.zero_point\", \"backbone.body.layer3.1.conv2.bias\", \"backbone.body.layer3.1.conv2.scale\", \"backbone.body.layer3.1.conv2.zero_point\", \"backbone.body.layer3.1.bn2.scale\", \"backbone.body.layer3.1.bn2.zero_point\", \"backbone.body.layer3.1.conv3.bias\", \"backbone.body.layer3.1.conv3.scale\", \"backbone.body.layer3.1.conv3.zero_point\", \"backbone.body.layer3.1.bn3.scale\", \"backbone.body.layer3.1.bn3.zero_point\", \"backbone.body.layer3.2.conv1.bias\", \"backbone.body.layer3.2.conv1.scale\", \"backbone.body.layer3.2.conv1.zero_point\", \"backbone.body.layer3.2.bn1.scale\", \"backbone.body.layer3.2.bn1.zero_point\", \"backbone.body.layer3.2.conv2.bias\", \"backbone.body.layer3.2.conv2.scale\", \"backbone.body.layer3.2.conv2.zero_point\", \"backbone.body.layer3.2.bn2.scale\", \"backbone.body.layer3.2.bn2.zero_point\", \"backbone.body.layer3.2.conv3.bias\", \"backbone.body.layer3.2.conv3.scale\", \"backbone.body.layer3.2.conv3.zero_point\", \"backbone.body.layer3.2.bn3.scale\", \"backbone.body.layer3.2.bn3.zero_point\", \"backbone.body.layer3.3.conv1.bias\", \"backbone.body.layer3.3.conv1.scale\", \"backbone.body.layer3.3.conv1.zero_point\", \"backbone.body.layer3.3.bn1.scale\", \"backbone.body.layer3.3.bn1.zero_point\", \"backbone.body.layer3.3.conv2.bias\", \"backbone.body.layer3.3.conv2.scale\", \"backbone.body.layer3.3.conv2.zero_point\", \"backbone.body.layer3.3.bn2.scale\", \"backbone.body.layer3.3.bn2.zero_point\", \"backbone.body.layer3.3.conv3.bias\", \"backbone.body.layer3.3.conv3.scale\", \"backbone.body.layer3.3.conv3.zero_point\", \"backbone.body.layer3.3.bn3.scale\", \"backbone.body.layer3.3.bn3.zero_point\", \"backbone.body.layer3.4.conv1.bias\", \"backbone.body.layer3.4.conv1.scale\", \"backbone.body.layer3.4.conv1.zero_point\", \"backbone.body.layer3.4.bn1.scale\", \"backbone.body.layer3.4.bn1.zero_point\", \"backbone.body.layer3.4.conv2.bias\", \"backbone.body.layer3.4.conv2.scale\", \"backbone.body.layer3.4.conv2.zero_point\", \"backbone.body.layer3.4.bn2.scale\", \"backbone.body.layer3.4.bn2.zero_point\", \"backbone.body.layer3.4.conv3.bias\", \"backbone.body.layer3.4.conv3.scale\", \"backbone.body.layer3.4.conv3.zero_point\", \"backbone.body.layer3.4.bn3.scale\", \"backbone.body.layer3.4.bn3.zero_point\", \"backbone.body.layer3.5.conv1.bias\", \"backbone.body.layer3.5.conv1.scale\", \"backbone.body.layer3.5.conv1.zero_point\", \"backbone.body.layer3.5.bn1.scale\", \"backbone.body.layer3.5.bn1.zero_point\", \"backbone.body.layer3.5.conv2.bias\", \"backbone.body.layer3.5.conv2.scale\", \"backbone.body.layer3.5.conv2.zero_point\", \"backbone.body.layer3.5.bn2.scale\", \"backbone.body.layer3.5.bn2.zero_point\", \"backbone.body.layer3.5.conv3.bias\", \"backbone.body.layer3.5.conv3.scale\", \"backbone.body.layer3.5.conv3.zero_point\", \"backbone.body.layer3.5.bn3.scale\", \"backbone.body.layer3.5.bn3.zero_point\", \"backbone.body.layer4.0.conv1.bias\", \"backbone.body.layer4.0.conv1.scale\", \"backbone.body.layer4.0.conv1.zero_point\", \"backbone.body.layer4.0.bn1.scale\", \"backbone.body.layer4.0.bn1.zero_point\", \"backbone.body.layer4.0.conv2.bias\", \"backbone.body.layer4.0.conv2.scale\", \"backbone.body.layer4.0.conv2.zero_point\", \"backbone.body.layer4.0.bn2.scale\", \"backbone.body.layer4.0.bn2.zero_point\", \"backbone.body.layer4.0.conv3.bias\", \"backbone.body.layer4.0.conv3.scale\", \"backbone.body.layer4.0.conv3.zero_point\", \"backbone.body.layer4.0.bn3.scale\", \"backbone.body.layer4.0.bn3.zero_point\", \"backbone.body.layer4.0.downsample.0.bias\", \"backbone.body.layer4.0.downsample.0.scale\", \"backbone.body.layer4.0.downsample.0.zero_point\", \"backbone.body.layer4.0.downsample.1.scale\", \"backbone.body.layer4.0.downsample.1.zero_point\", \"backbone.body.layer4.1.conv1.bias\", \"backbone.body.layer4.1.conv1.scale\", \"backbone.body.layer4.1.conv1.zero_point\", \"backbone.body.layer4.1.bn1.scale\", \"backbone.body.layer4.1.bn1.zero_point\", \"backbone.body.layer4.1.conv2.bias\", \"backbone.body.layer4.1.conv2.scale\", \"backbone.body.layer4.1.conv2.zero_point\", \"backbone.body.layer4.1.bn2.scale\", \"backbone.body.layer4.1.bn2.zero_point\", \"backbone.body.layer4.1.conv3.bias\", \"backbone.body.layer4.1.conv3.scale\", \"backbone.body.layer4.1.conv3.zero_point\", \"backbone.body.layer4.1.bn3.scale\", \"backbone.body.layer4.1.bn3.zero_point\", \"backbone.body.layer4.2.conv1.bias\", \"backbone.body.layer4.2.conv1.scale\", \"backbone.body.layer4.2.conv1.zero_point\", \"backbone.body.layer4.2.bn1.scale\", \"backbone.body.layer4.2.bn1.zero_point\", \"backbone.body.layer4.2.conv2.bias\", \"backbone.body.layer4.2.conv2.scale\", \"backbone.body.layer4.2.conv2.zero_point\", \"backbone.body.layer4.2.bn2.scale\", \"backbone.body.layer4.2.bn2.zero_point\", \"backbone.body.layer4.2.conv3.bias\", \"backbone.body.layer4.2.conv3.scale\", \"backbone.body.layer4.2.conv3.zero_point\", \"backbone.body.layer4.2.bn3.scale\", \"backbone.body.layer4.2.bn3.zero_point\", \"backbone.fpn.inner_blocks.0.0.scale\", \"backbone.fpn.inner_blocks.0.0.zero_point\", \"backbone.fpn.inner_blocks.1.0.scale\", \"backbone.fpn.inner_blocks.1.0.zero_point\", \"backbone.fpn.inner_blocks.2.0.scale\", \"backbone.fpn.inner_blocks.2.0.zero_point\", \"backbone.fpn.layer_blocks.0.0.scale\", \"backbone.fpn.layer_blocks.0.0.zero_point\", \"backbone.fpn.layer_blocks.1.0.scale\", \"backbone.fpn.layer_blocks.1.0.zero_point\", \"backbone.fpn.layer_blocks.2.0.scale\", \"backbone.fpn.layer_blocks.2.0.zero_point\", \"backbone.fpn.extra_blocks.p6.scale\", \"backbone.fpn.extra_blocks.p6.zero_point\", \"backbone.fpn.extra_blocks.p7.scale\", \"backbone.fpn.extra_blocks.p7.zero_point\", \"head.classification_head.conv.0.0.bias\", \"head.classification_head.conv.0.0.scale\", \"head.classification_head.conv.0.0.zero_point\", \"head.classification_head.conv.0.1.scale\", \"head.classification_head.conv.0.1.zero_point\", \"head.classification_head.conv.1.0.bias\", \"head.classification_head.conv.1.0.scale\", \"head.classification_head.conv.1.0.zero_point\", \"head.classification_head.conv.1.1.scale\", \"head.classification_head.conv.1.1.zero_point\", \"head.classification_head.conv.2.0.bias\", \"head.classification_head.conv.2.0.scale\", \"head.classification_head.conv.2.0.zero_point\", \"head.classification_head.conv.2.1.scale\", \"head.classification_head.conv.2.1.zero_point\", \"head.classification_head.conv.3.0.bias\", \"head.classification_head.conv.3.0.scale\", \"head.classification_head.conv.3.0.zero_point\", \"head.classification_head.conv.3.1.scale\", \"head.classification_head.conv.3.1.zero_point\", \"head.classification_head.cls_logits.scale\", \"head.classification_head.cls_logits.zero_point\", \"head.regression_head.conv.0.0.bias\", \"head.regression_head.conv.0.0.scale\", \"head.regression_head.conv.0.0.zero_point\", \"head.regression_head.conv.0.1.scale\", \"head.regression_head.conv.0.1.zero_point\", \"head.regression_head.conv.1.0.bias\", \"head.regression_head.conv.1.0.scale\", \"head.regression_head.conv.1.0.zero_point\", \"head.regression_head.conv.1.1.scale\", \"head.regression_head.conv.1.1.zero_point\", \"head.regression_head.conv.2.0.bias\", \"head.regression_head.conv.2.0.scale\", \"head.regression_head.conv.2.0.zero_point\", \"head.regression_head.conv.2.1.scale\", \"head.regression_head.conv.2.1.zero_point\", \"head.regression_head.conv.3.0.bias\", \"head.regression_head.conv.3.0.scale\", \"head.regression_head.conv.3.0.zero_point\", \"head.regression_head.conv.3.1.scale\", \"head.regression_head.conv.3.1.zero_point\", \"head.regression_head.bbox_reg.scale\", \"head.regression_head.bbox_reg.zero_point\". \n\tsize mismatch for backbone.body.conv1.weight: copying a param with shape torch.Size([64, 4, 7, 7]) from checkpoint, the shape in current model is torch.Size([64, 3, 7, 7]).\n\tWhile copying the parameter named \"backbone.body.layer1.0.conv1.weight\", whose dimensions in the model are torch.Size([64, 64, 1, 1]) and whose dimensions in the checkpoint are torch.Size([64, 64, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer1.0.conv2.weight\", whose dimensions in the model are torch.Size([64, 64, 3, 3]) and whose dimensions in the checkpoint are torch.Size([64, 64, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer1.0.conv3.weight\", whose dimensions in the model are torch.Size([256, 64, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 64, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer1.0.downsample.0.weight\", whose dimensions in the model are torch.Size([256, 64, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 64, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer1.1.conv1.weight\", whose dimensions in the model are torch.Size([64, 256, 1, 1]) and whose dimensions in the checkpoint are torch.Size([64, 256, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer1.1.conv2.weight\", whose dimensions in the model are torch.Size([64, 64, 3, 3]) and whose dimensions in the checkpoint are torch.Size([64, 64, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer1.1.conv3.weight\", whose dimensions in the model are torch.Size([256, 64, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 64, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer1.2.conv1.weight\", whose dimensions in the model are torch.Size([64, 256, 1, 1]) and whose dimensions in the checkpoint are torch.Size([64, 256, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer1.2.conv2.weight\", whose dimensions in the model are torch.Size([64, 64, 3, 3]) and whose dimensions in the checkpoint are torch.Size([64, 64, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer1.2.conv3.weight\", whose dimensions in the model are torch.Size([256, 64, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 64, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.0.conv1.weight\", whose dimensions in the model are torch.Size([128, 256, 1, 1]) and whose dimensions in the checkpoint are torch.Size([128, 256, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.0.conv2.weight\", whose dimensions in the model are torch.Size([128, 128, 3, 3]) and whose dimensions in the checkpoint are torch.Size([128, 128, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.0.conv3.weight\", whose dimensions in the model are torch.Size([512, 128, 1, 1]) and whose dimensions in the checkpoint are torch.Size([512, 128, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.0.downsample.0.weight\", whose dimensions in the model are torch.Size([512, 256, 1, 1]) and whose dimensions in the checkpoint are torch.Size([512, 256, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.1.conv1.weight\", whose dimensions in the model are torch.Size([128, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([128, 512, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.1.conv2.weight\", whose dimensions in the model are torch.Size([128, 128, 3, 3]) and whose dimensions in the checkpoint are torch.Size([128, 128, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.1.conv3.weight\", whose dimensions in the model are torch.Size([512, 128, 1, 1]) and whose dimensions in the checkpoint are torch.Size([512, 128, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.2.conv1.weight\", whose dimensions in the model are torch.Size([128, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([128, 512, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.2.conv2.weight\", whose dimensions in the model are torch.Size([128, 128, 3, 3]) and whose dimensions in the checkpoint are torch.Size([128, 128, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.2.conv3.weight\", whose dimensions in the model are torch.Size([512, 128, 1, 1]) and whose dimensions in the checkpoint are torch.Size([512, 128, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.3.conv1.weight\", whose dimensions in the model are torch.Size([128, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([128, 512, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.3.conv2.weight\", whose dimensions in the model are torch.Size([128, 128, 3, 3]) and whose dimensions in the checkpoint are torch.Size([128, 128, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer2.3.conv3.weight\", whose dimensions in the model are torch.Size([512, 128, 1, 1]) and whose dimensions in the checkpoint are torch.Size([512, 128, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.0.conv1.weight\", whose dimensions in the model are torch.Size([256, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 512, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.0.conv2.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.0.conv3.weight\", whose dimensions in the model are torch.Size([1024, 256, 1, 1]) and whose dimensions in the checkpoint are torch.Size([1024, 256, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.0.downsample.0.weight\", whose dimensions in the model are torch.Size([1024, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([1024, 512, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.1.conv1.weight\", whose dimensions in the model are torch.Size([256, 1024, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 1024, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.1.conv2.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.1.conv3.weight\", whose dimensions in the model are torch.Size([1024, 256, 1, 1]) and whose dimensions in the checkpoint are torch.Size([1024, 256, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.2.conv1.weight\", whose dimensions in the model are torch.Size([256, 1024, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 1024, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.2.conv2.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.2.conv3.weight\", whose dimensions in the model are torch.Size([1024, 256, 1, 1]) and whose dimensions in the checkpoint are torch.Size([1024, 256, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.3.conv1.weight\", whose dimensions in the model are torch.Size([256, 1024, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 1024, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.3.conv2.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.3.conv3.weight\", whose dimensions in the model are torch.Size([1024, 256, 1, 1]) and whose dimensions in the checkpoint are torch.Size([1024, 256, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.4.conv1.weight\", whose dimensions in the model are torch.Size([256, 1024, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 1024, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.4.conv2.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.4.conv3.weight\", whose dimensions in the model are torch.Size([1024, 256, 1, 1]) and whose dimensions in the checkpoint are torch.Size([1024, 256, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.5.conv1.weight\", whose dimensions in the model are torch.Size([256, 1024, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 1024, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.5.conv2.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer3.5.conv3.weight\", whose dimensions in the model are torch.Size([1024, 256, 1, 1]) and whose dimensions in the checkpoint are torch.Size([1024, 256, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer4.0.conv1.weight\", whose dimensions in the model are torch.Size([512, 1024, 1, 1]) and whose dimensions in the checkpoint are torch.Size([512, 1024, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer4.0.conv2.weight\", whose dimensions in the model are torch.Size([512, 512, 3, 3]) and whose dimensions in the checkpoint are torch.Size([512, 512, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer4.0.conv3.weight\", whose dimensions in the model are torch.Size([2048, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([2048, 512, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer4.0.downsample.0.weight\", whose dimensions in the model are torch.Size([2048, 1024, 1, 1]) and whose dimensions in the checkpoint are torch.Size([2048, 1024, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer4.1.conv1.weight\", whose dimensions in the model are torch.Size([512, 2048, 1, 1]) and whose dimensions in the checkpoint are torch.Size([512, 2048, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer4.1.conv2.weight\", whose dimensions in the model are torch.Size([512, 512, 3, 3]) and whose dimensions in the checkpoint are torch.Size([512, 512, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer4.1.conv3.weight\", whose dimensions in the model are torch.Size([2048, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([2048, 512, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer4.2.conv1.weight\", whose dimensions in the model are torch.Size([512, 2048, 1, 1]) and whose dimensions in the checkpoint are torch.Size([512, 2048, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer4.2.conv2.weight\", whose dimensions in the model are torch.Size([512, 512, 3, 3]) and whose dimensions in the checkpoint are torch.Size([512, 512, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.body.layer4.2.conv3.weight\", whose dimensions in the model are torch.Size([2048, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([2048, 512, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.fpn.inner_blocks.0.0.weight\", whose dimensions in the model are torch.Size([256, 512, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 512, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.fpn.inner_blocks.1.0.weight\", whose dimensions in the model are torch.Size([256, 1024, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 1024, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.fpn.inner_blocks.2.0.weight\", whose dimensions in the model are torch.Size([256, 2048, 1, 1]) and whose dimensions in the checkpoint are torch.Size([256, 2048, 1, 1]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.fpn.layer_blocks.0.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.fpn.layer_blocks.1.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.fpn.layer_blocks.2.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.fpn.extra_blocks.p6.weight\", whose dimensions in the model are torch.Size([256, 2048, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 2048, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"backbone.fpn.extra_blocks.p7.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"head.classification_head.conv.0.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"head.classification_head.conv.1.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"head.classification_head.conv.2.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"head.classification_head.conv.3.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tsize mismatch for head.classification_head.cls_logits.weight: copying a param with shape torch.Size([28, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([27, 256, 3, 3]).\n\tsize mismatch for head.classification_head.cls_logits.bias: copying a param with shape torch.Size([28]) from checkpoint, the shape in current model is torch.Size([27]).\n\tWhile copying the parameter named \"head.regression_head.conv.0.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"head.regression_head.conv.1.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"head.regression_head.conv.2.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"head.regression_head.conv.3.0.weight\", whose dimensions in the model are torch.Size([256, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([256, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).\n\tWhile copying the parameter named \"head.regression_head.bbox_reg.weight\", whose dimensions in the model are torch.Size([36, 256, 3, 3]) and whose dimensions in the checkpoint are torch.Size([36, 256, 3, 3]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',)."
          ]
        }
      ],
      "source": [
        "\n",
        "# Measure inference time for the original model\n",
        "if torch.cuda.is_available():\n",
        "    original_model = original_model.cuda()\n",
        "original_inference_time = measure_inference_time(original_model, dummy_input)\n",
        "print(f\"Original Model Inference Time: {original_inference_time:.6f} seconds\")\n",
        "\n",
        "# Load the quantized model from the saved state_dict\n",
        "quantized_model = create_model(num_classes=3)\n",
        "quantized_model.load_state_dict(torch.load(temp_path))\n",
        "\n",
        "# Measure inference time for the quantized model\n",
        "if torch.cuda.is_available():\n",
        "    quantized_model = quantized_model.cuda()\n",
        "quantized_inference_time = measure_inference_time(quantized_model, dummy_input)\n",
        "print(f\"Quantized Model Inference Time: {quantized_inference_time:.6f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s-xnaSy4nVvM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define your model creation function (adjust as per your model architecture)\n",
        "def create_models(num_classes):\n",
        "    # Replace with your actual model architecture\n",
        "    model = torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        torch.nn.Flatten(),\n",
        "        torch.nn.Linear(32 * 56 * 56, num_classes)\n",
        "    )\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_RRlrLyUnYD1"
      },
      "outputs": [],
      "source": [
        "# Load the quantized model from the saved state_dict\n",
        "# Use the correct model creation function for RetinaNet\n",
        "quantized_model = model\n",
        "quantized_model.load_state_dict(torch.load(temp_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "g8gv0PG1oBNK"
      },
      "outputs": [],
      "source": [
        "# Prepare dummy input data (example for an image input of shape (1, 3, 224, 224))\n",
        "dummy_input = torch.randn(1, 3, 224, 224)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8TVhabUVoGMm"
      },
      "outputs": [],
      "source": [
        "# Check if CUDA is available and use it if possible\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "quantized_inference_time = measure_inference_time(quantized_model, dummy_input, device)\n",
        "print(f\"Quantized Model Inference Time: {quantized_inference_time:.6f} seconds\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1DvKlUrWjyUKiZVdIrS7QshrlQac4Ld6A",
      "authorship_tag": "ABX9TyOf1monB6UAlrjUDAMzl+3V",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}